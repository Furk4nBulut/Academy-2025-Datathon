{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-05T07:15:54.568084Z",
     "start_time": "2025-05-05T07:11:26.848411Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, RandomizedSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from scipy.stats import uniform, randint, loguniform\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configuration\n",
    "DATASET_PATH = 'dataset/'\n",
    "TRAINING_DATA_PATH = DATASET_PATH + \"train.csv\"\n",
    "TESTING_DATA_PATH = DATASET_PATH + \"testFeatures.csv\"\n",
    "LOW_QUANTILE = 0.02\n",
    "UP_QUANTILE = 0.99\n",
    "CAT_THRESHOLD = 5\n",
    "CAR_THRESHOLD = 10\n",
    "CORRELATION_THRESHOLD = 0.40\n",
    "CAT_LENGTH = 8\n",
    "NUM_METHOD = \"median\"\n",
    "\n",
    "# Helper Functions\n",
    "def check_df(dataframe):\n",
    "    print(\"##################### Shape #####################\")\n",
    "    print(dataframe.shape)\n",
    "    print(\"##################### Types #####################\")\n",
    "    print(dataframe.dtypes)\n",
    "    print(\"##################### Head #####################\")\n",
    "    print(dataframe.head(3))\n",
    "    print(\"##################### Tail #####################\")\n",
    "    print(dataframe.tail(3))\n",
    "    print(\"##################### NA #####################\")\n",
    "    print(dataframe.isnull().sum())\n",
    "    print(\"##################### Quantiles #####################\")\n",
    "    print(dataframe.quantile([0, 0.05, 0.50, 0.95, 0.99, 1]).T)\n",
    "\n",
    "def grab_col_names(dataframe, cat_th=CAT_THRESHOLD, car_th=CAR_THRESHOLD):\n",
    "    cat_cols = [col for col in dataframe.columns if dataframe[col].dtype == \"O\"]\n",
    "    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() < cat_th and dataframe[col].dtype != \"O\"]\n",
    "    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() > car_th and dataframe[col].dtype == \"O\"]\n",
    "    cat_cols = cat_cols + num_but_cat\n",
    "    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n",
    "    num_cols = [col for col in dataframe.columns if dataframe[col].dtype != \"O\"]\n",
    "    num_cols = [col for col in num_cols if col not in num_but_cat]\n",
    "    print(f\"Observations: {dataframe.shape[0]}\")\n",
    "    print(f\"Variables: {dataframe.shape[1]}\")\n",
    "    print(f'cat_cols: {len(cat_cols)}')\n",
    "    print(f'num_cols: {len(num_cols)}')\n",
    "    print(f'cat_but_car: {len(cat_but_car)}')\n",
    "    print(f'num_but_cat: {len(num_but_cat)}')\n",
    "    return cat_cols, cat_but_car, num_cols\n",
    "\n",
    "def cat_summary(dataframe, col_name, plot=False):\n",
    "    print(pd.DataFrame({col_name: dataframe[col_name].value_counts(),\n",
    "                        \"Ratio\": 100 * dataframe[col_name].value_counts() / len(dataframe)}))\n",
    "    if plot:\n",
    "        sns.countplot(x=dataframe[col_name], data=dataframe)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.savefig(f'{col_name}_countplot.png')\n",
    "        plt.close()\n",
    "\n",
    "def num_summary(dataframe, numerical_col, plot=False):\n",
    "    quantiles = [0.05, 0.10, 0.50, 0.90, 0.95, 0.99]\n",
    "    print(dataframe[numerical_col].describe(quantiles).T)\n",
    "    if plot:\n",
    "        dataframe[numerical_col].hist(bins=50)\n",
    "        plt.xlabel(numerical_col)\n",
    "        plt.title(numerical_col)\n",
    "        plt.savefig(f'{numerical_col}_histogram.png')\n",
    "        plt.close()\n",
    "    print(\"#####################################\")\n",
    "\n",
    "def target_summary_with_cat(dataframe, target, categorical_col):\n",
    "    print(pd.DataFrame({\"TARGET_MEAN\": dataframe.groupby(categorical_col)[target].mean()}), end=\"\\n\\n\\n\")\n",
    "\n",
    "def high_correlated_cols(dataframe, plot=False, corr_th=CORRELATION_THRESHOLD):\n",
    "    corr = dataframe.corr(numeric_only=True)\n",
    "    cor_matrix = corr.abs()\n",
    "    upper_triangle_matrix = cor_matrix.where(np.triu(np.ones(cor_matrix.shape), k=1).astype(bool))\n",
    "    drop_list = [col for col in upper_triangle_matrix.columns if any(upper_triangle_matrix[col] > corr_th)]\n",
    "    if plot:\n",
    "        sns.heatmap(corr, cmap=\"RdBu\", annot=True)\n",
    "        plt.savefig('correlation_heatmap.png')\n",
    "        plt.close()\n",
    "    return drop_list\n",
    "\n",
    "def outlier_thresholds(dataframe, variable, low_quantile=LOW_QUANTILE, up_quantile=UP_QUANTILE):\n",
    "    q1 = dataframe[variable].quantile(low_quantile)\n",
    "    q3 = dataframe[variable].quantile(up_quantile)\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "    return lower_bound, upper_bound\n",
    "\n",
    "def category_outlier_thresholds(df, col, category_col='ürün kategorisi'):\n",
    "    thresholds = {}\n",
    "    for category in df[category_col].unique():\n",
    "        cat_df = df[df[category_col] == category]\n",
    "        low, up = outlier_thresholds(cat_df, col)\n",
    "        thresholds[category] = (low, up)\n",
    "    return thresholds\n",
    "\n",
    "def check_outlier(dataframe, col_name):\n",
    "    low_limit, up_limit = outlier_thresholds(dataframe, col_name)\n",
    "    return dataframe[(dataframe[col_name] > up_limit) | (dataframe[col_name] < low_limit)].any(axis=None)\n",
    "\n",
    "def replace_with_thresholds(dataframe, variable, category_col='ürün kategorisi'):\n",
    "    thresholds = category_outlier_thresholds(dataframe, variable, category_col)\n",
    "    for category, (low, up) in thresholds.items():\n",
    "        dataframe.loc[(dataframe[category_col] == category) & (dataframe[variable] < low), variable] = low\n",
    "        dataframe.loc[(dataframe[category_col] == category) & (dataframe[variable] > up), variable] = up\n",
    "    return dataframe\n",
    "\n",
    "def missing_values_table(dataframe, na_name=False):\n",
    "    na_columns = [col for col in dataframe.columns if dataframe[col].isnull().sum() > 0]\n",
    "    n_miss = dataframe[na_columns].isnull().sum().sort_values(ascending=False)\n",
    "    ratio = (dataframe[na_columns].isnull().sum() / dataframe.shape[0] * 100).sort_values(ascending=False)\n",
    "    missing_df = pd.concat([n_miss, np.round(ratio, 2)], axis=1, keys=['n_miss', 'ratio'])\n",
    "    print(missing_df, end=\"\\n\")\n",
    "    if na_name:\n",
    "        return na_columns\n",
    "\n",
    "def remove_missing_values(dataframe):\n",
    "    print(\"##################### Missing Values Before #####################\")\n",
    "    print(dataframe.isnull().sum())\n",
    "    dataframe_cleaned = dataframe.dropna()\n",
    "    print(\"##################### Missing Values After #####################\")\n",
    "    print(dataframe_cleaned.isnull().sum())\n",
    "    return dataframe_cleaned\n",
    "\n",
    "def quick_missing_imp(data, num_method=NUM_METHOD, cat_length=CAT_LENGTH, target=\"ürün fiyatı\"):\n",
    "    variables_with_na = [col for col in data.columns if data[col].isnull().sum() > 0]\n",
    "    temp_target = data[target] if target in data.columns else None\n",
    "    print(\"# BEFORE\")\n",
    "    print(data[variables_with_na].isnull().sum(), \"\\n\")\n",
    "    data = data.apply(lambda x: x.fillna(x.mode()[0]) if (x.dtype == \"O\" and len(x.unique()) <= cat_length) else x, axis=0)\n",
    "    if num_method == \"mean\":\n",
    "        data = data.apply(lambda x: x.fillna(x.mean()) if x.dtype != \"O\" else x, axis=0)\n",
    "    elif num_method == \"median\":\n",
    "        data = data.apply(lambda x: x.fillna(x.median()) if x.dtype != \"O\" else x, axis=0)\n",
    "    if temp_target is not None:\n",
    "        data[target] = temp_target\n",
    "    print(\"# AFTER\")\n",
    "    print(\"Categorical variables filled with mode\")\n",
    "    print(f\"Numerical variables filled with {num_method}\")\n",
    "    print(data[variables_with_na].isnull().sum(), \"\\n\")\n",
    "    return data\n",
    "\n",
    "def rare_analyser(dataframe, target, cat_cols):\n",
    "    for col in cat_cols:\n",
    "        print(col, \":\", len(dataframe[col].value_counts()))\n",
    "        print(pd.DataFrame({\"COUNT\": dataframe[col].value_counts(),\n",
    "                            \"RATIO\": dataframe[col].value_counts() / len(dataframe),\n",
    "                            \"TARGET_MEAN\": dataframe.groupby(col)[target].mean()}), end=\"\\n\\n\\n\")\n",
    "\n",
    "def rare_encoder(dataframe, rare_perc):\n",
    "    temp_df = dataframe.copy()\n",
    "    rare_columns = [col for col in temp_df.columns if temp_df[col].dtype == 'O'\n",
    "                    and (temp_df[col].value_counts() / len(temp_df) < rare_perc).any(axis=None)]\n",
    "    for var in rare_columns:\n",
    "        tmp = temp_df[var].value_counts() / len(temp_df)\n",
    "        rare_labels = tmp[tmp < rare_perc].index\n",
    "        temp_df[var] = np.where(temp_df[var].isin(rare_labels), 'Rare', temp_df[var])\n",
    "    return temp_df\n",
    "\n",
    "def label_encoder(dataframe, binary_col):\n",
    "    labelencoder = LabelEncoder()\n",
    "    dataframe[binary_col] = labelencoder.fit_transform(dataframe[binary_col])\n",
    "    return dataframe\n",
    "\n",
    "def one_hot_encoder(dataframe, categorical_cols, drop_first=False):\n",
    "    return pd.get_dummies(dataframe, columns=categorical_cols, drop_first=drop_first)\n",
    "\n",
    "# DataLoader Class\n",
    "class DataLoader:\n",
    "    def __init__(self, training_data_path, testing_data_path):\n",
    "        self.training_data_path = training_data_path\n",
    "        self.testing_data_path = testing_data_path\n",
    "        print(\"Initializing DataLoader...\")\n",
    "\n",
    "    def get_data(self):\n",
    "        print(\"Loading data...\")\n",
    "        train = pd.read_csv(self.training_data_path)\n",
    "        test = pd.read_csv(self.testing_data_path)\n",
    "        df = pd.concat([train, test], ignore_index=True)\n",
    "        df = df.reset_index(drop=True)\n",
    "        print(\"Data loaded successfully.\")\n",
    "        return df\n",
    "\n",
    "# DataPreprocessing Class\n",
    "class DataPreprocessing:\n",
    "    def __init__(self, dataframe):\n",
    "        self.df = dataframe.copy()\n",
    "\n",
    "    def preprocess(self, is_test_only=False):\n",
    "        self.handle_outliers()\n",
    "        self.handle_missing_values()\n",
    "        self.feature_engineering()\n",
    "        self.drop_unnecessary_columns()\n",
    "        self.encode_features()\n",
    "\n",
    "        if is_test_only:\n",
    "            test_data = self.df[self.df['ürün fiyatı'].isnull()].drop('ürün fiyatı', axis=1)\n",
    "            test_ids = test_data[\"id\"].copy()\n",
    "            test_data = test_data.drop(columns=['id'])\n",
    "            return test_data, test_ids\n",
    "        else:\n",
    "            train_data = self.df[self.df['ürün fiyatı'].notnull()]\n",
    "            train_data = train_data.drop(columns=['id'])\n",
    "            X = train_data.drop('ürün fiyatı', axis=1)\n",
    "            y = train_data['ürün fiyatı']\n",
    "            y_log = np.log1p(y)  # Log transformation for target\n",
    "            X_train, X_val, y_train, y_val = train_test_split(X, y_log, test_size=0.10, random_state=42)\n",
    "            return X_train, X_val, y_train, y_val, y  # Return original y for error analysis\n",
    "\n",
    "    def handle_outliers(self):\n",
    "        num_cols = self.df.select_dtypes(include=np.number).columns\n",
    "        num_cols = [col for col in num_cols if col != 'ürün fiyatı']\n",
    "        for col in num_cols:\n",
    "            if check_outlier(self.df, col):\n",
    "                self.df = replace_with_thresholds(self.df, col)\n",
    "\n",
    "    def handle_missing_values(self):\n",
    "        num_cols = self.df.select_dtypes(include=np.number).columns\n",
    "        num_cols = [col for col in num_cols if col != 'ürün fiyatı']\n",
    "        self.df[num_cols] = self.df.groupby('ürün kategorisi')[num_cols].transform(lambda x: x.fillna(x.median()))\n",
    "        cat_cols = self.df.select_dtypes(include='object').columns\n",
    "        for col in cat_cols:\n",
    "            self.df[col] = self.df[col].fillna(self.df[col].mode()[0])\n",
    "\n",
    "    def feature_engineering(self):\n",
    "        # Time-based features\n",
    "        self.df['tarih'] = pd.to_datetime(self.df['tarih'])\n",
    "        self.df['ay'] = self.df['tarih'].dt.month\n",
    "        self.df['çeyrek'] = self.df['tarih'].dt.quarter\n",
    "        self.df['haftanın_günü'] = self.df['tarih'].dt.weekday\n",
    "\n",
    "        # Product and category-based features\n",
    "        self.df['besin_değeri_log'] = np.log1p(self.df['ürün besin değeri'])\n",
    "        self.df['kategori_ortalama_besin'] = self.df.groupby('ürün kategorisi')['ürün besin değeri'].transform('mean')\n",
    "        self.df['ürün_ortalama_fiyat'] = self.df.groupby('ürün')['ürün fiyatı'].transform('mean')\n",
    "        self.df['kategori_fiyat_std'] = self.df.groupby('ürün kategorisi')['ürün fiyatı'].transform('std')\n",
    "        self.df['besin_değeri_kategori'] = pd.qcut(self.df['ürün besin değeri'], q=3, labels=['düşük', 'orta', 'yüksek'])\n",
    "        self.df['ürün_freq'] = self.df['ürün'].map(self.df['ürün'].value_counts() / len(self.df))\n",
    "\n",
    "    def drop_unnecessary_columns(self):\n",
    "        columns_to_drop = ['ürün üretim yeri', 'market', 'şehir']\n",
    "        self.df.drop(columns=[col for col in columns_to_drop if col in self.df.columns], inplace=True)\n",
    "\n",
    "    def encode_features(self):\n",
    "        cat_cols, cat_but_car, num_cols = grab_col_names(self.df)\n",
    "        binary_cols = [col for col in cat_cols if self.df[col].nunique() <= 3]\n",
    "        for col in binary_cols:\n",
    "            self.df = label_encoder(self.df, col)\n",
    "        high_cardinality_cols = cat_but_car + [col for col in cat_cols if col not in binary_cols]\n",
    "        for col in high_cardinality_cols:\n",
    "            if col in self.df.columns:\n",
    "                train_data = self.df[self.df['ürün fiyatı'].notnull()]\n",
    "                target_means = train_data.groupby(col)['ürün fiyatı'].mean()\n",
    "                self.df[col] = self.df[col].map(target_means).fillna(train_data['ürün fiyatı'].mean())\n",
    "        remaining_cat_cols = [col for col in cat_cols if col not in binary_cols and col not in high_cardinality_cols]\n",
    "        if remaining_cat_cols:\n",
    "            self.df = one_hot_encoder(self.df, remaining_cat_cols, drop_first=True)\n",
    "        remaining_object_cols = self.df.select_dtypes(include='object').columns.tolist()\n",
    "        if remaining_object_cols:\n",
    "            raise ValueError(f\"Categorical columns not fully encoded: {remaining_object_cols}\")\n",
    "\n",
    "# HyperTuner Class\n",
    "class HyperTuner:\n",
    "    def __init__(self):\n",
    "        self.param_grids = {\n",
    "     \"CatBoost\": {\n",
    "    'iterations': [100],  # Sabit kalabilir, zaten yeterli\n",
    "    'learning_rate': [0.01, 0.02],  # 0.01 iyi, belki biraz daha hızlı eğitim için 0.02 de denenebilir\n",
    "    'depth': [6, 8],  # 8 başarılı, 6 alternatif olarak yeterli\n",
    "    'l2_leaf_reg': [5, 7, 9],  # 7 iyi, biraz etrafı denenebilir\n",
    "    'bagging_temperature': [0.5, 1],  # 1 işe yarıyor, ama 0.5 ile genel performans kontrolü\n",
    "    'border_count': [64, 128],  # 64 işe yarıyor, 128 ile karşılaştırma yapılabilir\n",
    "    'grow_policy': ['Lossguide', 'Depthwise']  # 'Lossguide' işe yarıyor, sadece bir alternatif bırak\n",
    "}\n",
    ",\n",
    "            \"XGBoost\": {\n",
    "                'n_estimators': randint(50, 200),\n",
    "                'learning_rate': loguniform(0.005, 0.2),\n",
    "                'max_depth': [3, 5, 7, 9],\n",
    "                'subsample': uniform(0.6, 0.4),\n",
    "                'colsample_bytree': uniform(0.6, 0.4)\n",
    "            },\n",
    "            \"LightGBM\": {\n",
    "                'n_estimators': randint(50, 200),\n",
    "                'learning_rate': loguniform(0.005, 0.2),\n",
    "                'num_leaves': randint(20, 50),\n",
    "                'max_depth': [3, 5, 7, -1],\n",
    "                'subsample': uniform(0.6, 0.4),\n",
    "                'colsample_bytree': uniform(0.6, 0.4)\n",
    "            }\n",
    "        }\n",
    "        self.models = {\n",
    "            \"CatBoost\": CatBoostRegressor(silent=True, random_state=17),\n",
    "            \"XGBoost\": XGBRegressor(objective='reg:squarederror', random_state=17),\n",
    "            \"LightGBM\": LGBMRegressor(random_state=17),\n",
    "        }\n",
    "\n",
    "    def tune_model(self, model_name, X, y):\n",
    "        if model_name not in self.models:\n",
    "            raise ValueError(f\"Model {model_name} not found in models list.\")\n",
    "        model = self.models[model_name]\n",
    "        param_grid = self.param_grids[model_name]\n",
    "        search = RandomizedSearchCV(\n",
    "            estimator=model,\n",
    "            param_distributions=param_grid,\n",
    "            n_iter=10,\n",
    "            cv=3,\n",
    "            scoring='neg_mean_absolute_error',\n",
    "            n_jobs=-1,\n",
    "            random_state=42\n",
    "        )\n",
    "        search.fit(X, y)\n",
    "        return search.best_estimator_, search.best_params_\n",
    "\n",
    "# ModelEvaluator Class\n",
    "class ModelEvaluator:\n",
    "    def __init__(self, output_dir=\"predictions\"):\n",
    "        self.model_names = [\n",
    "            \"CatBoost\",\n",
    "        ]\n",
    "        self.best_model_name = \"CatBoost\"\n",
    "        self.rmse_scores = {}\n",
    "        self.mae_scores = {}\n",
    "        self.best_params = {}\n",
    "        self.tuner = HyperTuner()\n",
    "        self.trained_models = {}\n",
    "        self.output_dir = output_dir\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "\n",
    "    def evaluate_models(self, X, y, X_val, y_val, y_val_original):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=17)\n",
    "        print(\"Evaluating models with hyperparameter tuning...\")\n",
    "        for name in self.model_names:\n",
    "            best_model, best_params = self.tuner.tune_model(name, X_train, y_train)\n",
    "            self.trained_models[name] = best_model\n",
    "            self.best_params[name] = best_params\n",
    "            rmse = np.mean(np.sqrt(-cross_val_score(best_model, X, y, cv=5, scoring=\"neg_mean_squared_error\")))\n",
    "            self.rmse_scores[name] = rmse\n",
    "            mae = np.mean(-cross_val_score(best_model, X, y, cv=5, scoring=\"neg_mean_absolute_error\"))\n",
    "            self.mae_scores[name] = mae\n",
    "            print(f\"RMSE: {round(rmse, 4)} | MAE: {round(mae, 4)} ({name})\")\n",
    "            if self.best_params[name]:\n",
    "                print(f\"Best parameters: {self.best_params[name]}\")\n",
    "            # Category-based error analysis\n",
    "            y_pred_val = np.expm1(best_model.predict(X_val))\n",
    "            val_data = X_val.copy()\n",
    "            val_data['y_true'] = y_val_original\n",
    "            val_data['y_pred'] = y_pred_val\n",
    "            print(f\"\\nCategory-based MAE for {name}:\")\n",
    "            print(val_data.groupby('ürün kategorisi').apply(lambda x: mean_absolute_error(x['y_true'], x['y_pred'])))\n",
    "\n",
    "        # Stacking Ensemble\n",
    "        print(\"\\nTraining Stacking Ensemble...\")\n",
    "        estimators = [(name, self.trained_models[name]) for name in self.model_names]\n",
    "        stacking_model = StackingRegressor(estimators=estimators, final_estimator=LinearRegression())\n",
    "        stacking_model.fit(X_train, y_train)\n",
    "        self.trained_models['Stacking'] = stacking_model\n",
    "        rmse = np.mean(np.sqrt(-cross_val_score(stacking_model, X, y, cv=5, scoring=\"neg_mean_squared_error\")))\n",
    "        self.rmse_scores['Stacking'] = rmse\n",
    "        mae = np.mean(-cross_val_score(stacking_model, X, y, cv=5, scoring=\"neg_mean_absolute_error\"))\n",
    "        self.mae_scores['Stacking'] = mae\n",
    "        print(f\"RMSE: {round(rmse, 4)} | MAE: {round(mae, 4)} (Stacking)\")\n",
    "\n",
    "        best_model_mae = min(self.mae_scores, key=self.mae_scores.get)\n",
    "        print(f\"\\nBest model based on MAE: {best_model_mae} (MAE: {round(self.mae_scores[best_model_mae], 4)})\")\n",
    "        best_model_rmse = min(self.rmse_scores, key=self.rmse_scores.get)\n",
    "        print(f\"Best model based on RMSE: {best_model_rmse} (RMSE: {round(self.rmse_scores[best_model_rmse], 4)})\")\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "    def train_and_predict(self, X_train, y_train, X_test, test_ids, output_file=\"submission.csv\"):\n",
    "        print(f\"\\n⏳ Training {self.best_model_name} model...\")\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "        best_model, best_params = self.tuner.tune_model(self.best_model_name, X_train, y_train)\n",
    "        self.best_params[self.best_model_name] = best_params\n",
    "        best_model.fit(X_train, y_train)\n",
    "        training_time = time.time() - start_time\n",
    "        print(f\"✅ Training completed in {training_time:.2f} seconds\")\n",
    "        print(f\"🏆 Best parameters: {best_params}\")\n",
    "        predictions = np.expm1(best_model.predict(X_test))  # Inverse log transformation\n",
    "        submission_df = pd.DataFrame({\n",
    "            \"id\": test_ids.astype(int),\n",
    "            \"ürün fiyatı\": predictions.astype(float)\n",
    "        })\n",
    "        output_path = os.path.join(self.output_dir, output_file)\n",
    "        submission_df.to_csv(output_path, index=False, float_format='%.4f')\n",
    "        print(f\"\\n📁 Predictions saved to '{output_path}'\")\n",
    "        print(f\"Sample predictions:\\n{submission_df.head()}\")\n",
    "        return predictions\n",
    "\n",
    "    def get_rmse_scores(self):\n",
    "        return self.rmse_scores\n",
    "\n",
    "    def get_mae_scores(self):\n",
    "        return self.mae_scores\n",
    "\n",
    "    def get_best_params(self):\n",
    "        return self.best_params\n",
    "\n",
    "# Main Function\n",
    "def main():\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    print(\"🚀 Starting product price prediction pipeline...\")\n",
    "    print(\"\\n📂 Loading data...\")\n",
    "    data_loader = DataLoader(TRAINING_DATA_PATH, TESTING_DATA_PATH)\n",
    "    combined_df = data_loader.get_data()\n",
    "    print(f\"✅ Data loaded. Shape: {combined_df.shape}\")\n",
    "    print(\"\\n🔧 Preprocessing data...\")\n",
    "    preprocessor = DataPreprocessing(combined_df)\n",
    "    X_train, X_val, y_train, y_val, y_val_original = preprocessor.preprocess()\n",
    "    print(f\"✅ Training data prepared. Features: {X_train.shape[1]}, Samples: {X_train.shape[0]}\")\n",
    "    X_test_submission, test_ids = preprocessor.preprocess(is_test_only=True)\n",
    "    print(f\"✅ Test data prepared. Samples: {X_test_submission.shape[0]}\")\n",
    "    print(\"\\n🧪 Evaluating models...\")\n",
    "    evaluator = ModelEvaluator(output_dir=\"predictions/Boosting\")\n",
    "    evaluator.evaluate_models(X_train, y_train, X_val, y_val, y_val_original)\n",
    "    print(\"\\n🔮 Making final predictions...\")\n",
    "    predictions = evaluator.train_and_predict(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_test_submission,\n",
    "        test_ids,\n",
    "        output_file=\"submission.csv\"\n",
    "    )\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\n🎉 Pipeline completed in {total_time:.2f} seconds!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting product price prediction pipeline...\n",
      "\n",
      "📂 Loading data...\n",
      "Initializing DataLoader...\n",
      "Loading data...\n",
      "Data loaded successfully.\n",
      "✅ Data loaded. Shape: (273024, 9)\n",
      "\n",
      "🔧 Preprocessing data...\n",
      "Observations: 273024\n",
      "Variables: 15\n",
      "cat_cols: 4\n",
      "num_cols: 10\n",
      "cat_but_car: 1\n",
      "num_but_cat: 3\n",
      "✅ Training data prepared. Features: 13, Samples: 204768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19324/1562452927.py:118: FutureWarning: 'any' with datetime64 dtypes is deprecated and will raise in a future version. Use (obj != pd.Timestamp(0)).any() instead.\n",
      "  return dataframe[(dataframe[col_name] > up_limit) | (dataframe[col_name] < low_limit)].any(axis=None)\n",
      "/tmp/ipykernel_19324/1562452927.py:118: FutureWarning: 'any' with datetime64 dtypes is deprecated and will raise in a future version. Use (obj != pd.Timestamp(0)).any() instead.\n",
      "  return dataframe[(dataframe[col_name] > up_limit) | (dataframe[col_name] < low_limit)].any(axis=None)\n",
      "/tmp/ipykernel_19324/1562452927.py:118: FutureWarning: 'any' with datetime64 dtypes is deprecated and will raise in a future version. Use (obj != pd.Timestamp(0)).any() instead.\n",
      "  return dataframe[(dataframe[col_name] > up_limit) | (dataframe[col_name] < low_limit)].any(axis=None)\n",
      "/tmp/ipykernel_19324/1562452927.py:118: FutureWarning: 'any' with datetime64 dtypes is deprecated and will raise in a future version. Use (obj != pd.Timestamp(0)).any() instead.\n",
      "  return dataframe[(dataframe[col_name] > up_limit) | (dataframe[col_name] < low_limit)].any(axis=None)\n",
      "/tmp/ipykernel_19324/1562452927.py:118: FutureWarning: 'any' with datetime64 dtypes is deprecated and will raise in a future version. Use (obj != pd.Timestamp(0)).any() instead.\n",
      "  return dataframe[(dataframe[col_name] > up_limit) | (dataframe[col_name] < low_limit)].any(axis=None)\n",
      "/tmp/ipykernel_19324/1562452927.py:118: FutureWarning: 'any' with datetime64 dtypes is deprecated and will raise in a future version. Use (obj != pd.Timestamp(0)).any() instead.\n",
      "  return dataframe[(dataframe[col_name] > up_limit) | (dataframe[col_name] < low_limit)].any(axis=None)\n",
      "/tmp/ipykernel_19324/1562452927.py:118: FutureWarning: 'any' with datetime64 dtypes is deprecated and will raise in a future version. Use (obj != pd.Timestamp(0)).any() instead.\n",
      "  return dataframe[(dataframe[col_name] > up_limit) | (dataframe[col_name] < low_limit)].any(axis=None)\n",
      "/tmp/ipykernel_19324/1562452927.py:118: FutureWarning: 'any' with datetime64 dtypes is deprecated and will raise in a future version. Use (obj != pd.Timestamp(0)).any() instead.\n",
      "  return dataframe[(dataframe[col_name] > up_limit) | (dataframe[col_name] < low_limit)].any(axis=None)\n",
      "/tmp/ipykernel_19324/1562452927.py:118: FutureWarning: 'any' with datetime64 dtypes is deprecated and will raise in a future version. Use (obj != pd.Timestamp(0)).any() instead.\n",
      "  return dataframe[(dataframe[col_name] > up_limit) | (dataframe[col_name] < low_limit)].any(axis=None)\n",
      "/tmp/ipykernel_19324/1562452927.py:118: FutureWarning: 'any' with datetime64 dtypes is deprecated and will raise in a future version. Use (obj != pd.Timestamp(0)).any() instead.\n",
      "  return dataframe[(dataframe[col_name] > up_limit) | (dataframe[col_name] < low_limit)].any(axis=None)\n",
      "/tmp/ipykernel_19324/1562452927.py:118: FutureWarning: 'any' with datetime64 dtypes is deprecated and will raise in a future version. Use (obj != pd.Timestamp(0)).any() instead.\n",
      "  return dataframe[(dataframe[col_name] > up_limit) | (dataframe[col_name] < low_limit)].any(axis=None)\n",
      "/tmp/ipykernel_19324/1562452927.py:118: FutureWarning: 'any' with datetime64 dtypes is deprecated and will raise in a future version. Use (obj != pd.Timestamp(0)).any() instead.\n",
      "  return dataframe[(dataframe[col_name] > up_limit) | (dataframe[col_name] < low_limit)].any(axis=None)\n",
      "/tmp/ipykernel_19324/1562452927.py:118: FutureWarning: 'any' with datetime64 dtypes is deprecated and will raise in a future version. Use (obj != pd.Timestamp(0)).any() instead.\n",
      "  return dataframe[(dataframe[col_name] > up_limit) | (dataframe[col_name] < low_limit)].any(axis=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations: 273024\n",
      "Variables: 15\n",
      "cat_cols: 3\n",
      "num_cols: 12\n",
      "cat_but_car: 0\n",
      "num_but_cat: 3\n",
      "✅ Test data prepared. Samples: 45504\n",
      "\n",
      "🧪 Evaluating models...\n",
      "Evaluating models with hyperparameter tuning...\n",
      "RMSE: 0.2893 | MAE: 0.2224 (CatBoost)\n",
      "Best parameters: {'learning_rate': 0.01, 'l2_leaf_reg': 7, 'iterations': 100, 'grow_policy': 'Lossguide', 'depth': 8, 'border_count': 64, 'bagging_temperature': 1}\n",
      "\n",
      "Category-based MAE for CatBoost:\n",
      "ürün kategorisi\n",
      "9.976630      2.555764\n",
      "10.420985     1.792725\n",
      "15.534468     3.423504\n",
      "26.588236     7.292074\n",
      "31.349666     8.982162\n",
      "36.961375    15.158272\n",
      "dtype: float64\n",
      "\n",
      "Training Stacking Ensemble...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19324/1562452927.py:368: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  print(val_data.groupby('ürün kategorisi').apply(lambda x: mean_absolute_error(x['y_true'], x['y_pred'])))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.0856 | MAE: 0.0647 (Stacking)\n",
      "\n",
      "Best model based on MAE: Stacking (MAE: 0.0647)\n",
      "Best model based on RMSE: Stacking (RMSE: 0.0856)\n",
      "\n",
      "🔮 Making final predictions...\n",
      "\n",
      "⏳ Training CatBoost model...\n",
      "✅ Training completed in 67.37 seconds\n",
      "🏆 Best parameters: {'learning_rate': 0.01, 'l2_leaf_reg': 7, 'iterations': 100, 'grow_policy': 'Lossguide', 'depth': 8, 'border_count': 64, 'bagging_temperature': 1}\n",
      "\n",
      "📁 Predictions saved to 'predictions/Boosting/submission.csv'\n",
      "Sample predictions:\n",
      "        id  ürün fiyatı\n",
      "227520   0    39.523662\n",
      "227521   1    23.880532\n",
      "227522   2    25.886491\n",
      "227523   3    19.666484\n",
      "227524   4    29.171648\n",
      "\n",
      "🎉 Pipeline completed in 267.67 seconds!\n"
     ]
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
