{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-05T07:15:54.568084Z",
     "start_time": "2025-05-05T07:11:26.848411Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, RandomizedSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from scipy.stats import uniform, randint, loguniform\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configuration\n",
    "DATASET_PATH = 'dataset/'\n",
    "TRAINING_DATA_PATH = DATASET_PATH + \"train.csv\"\n",
    "TESTING_DATA_PATH = DATASET_PATH + \"testFeatures.csv\"\n",
    "LOW_QUANTILE = 0.02\n",
    "UP_QUANTILE = 0.99\n",
    "CAT_THRESHOLD = 5\n",
    "CAR_THRESHOLD = 10\n",
    "CORRELATION_THRESHOLD = 0.40\n",
    "CAT_LENGTH = 8\n",
    "NUM_METHOD = \"median\"\n",
    "\n",
    "# Helper Functions\n",
    "def check_df(dataframe):\n",
    "    print(\"##################### Shape #####################\")\n",
    "    print(dataframe.shape)\n",
    "    print(\"##################### Types #####################\")\n",
    "    print(dataframe.dtypes)\n",
    "    print(\"##################### Head #####################\")\n",
    "    print(dataframe.head(3))\n",
    "    print(\"##################### Tail #####################\")\n",
    "    print(dataframe.tail(3))\n",
    "    print(\"##################### NA #####################\")\n",
    "    print(dataframe.isnull().sum())\n",
    "    print(\"##################### Quantiles #####################\")\n",
    "    print(dataframe.quantile([0, 0.05, 0.50, 0.95, 0.99, 1]).T)\n",
    "\n",
    "def grab_col_names(dataframe, cat_th=CAT_THRESHOLD, car_th=CAR_THRESHOLD):\n",
    "    cat_cols = [col for col in dataframe.columns if dataframe[col].dtype == \"O\"]\n",
    "    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() < cat_th and dataframe[col].dtype != \"O\"]\n",
    "    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() > car_th and dataframe[col].dtype == \"O\"]\n",
    "    cat_cols = cat_cols + num_but_cat\n",
    "    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n",
    "    num_cols = [col for col in dataframe.columns if dataframe[col].dtype != \"O\"]\n",
    "    num_cols = [col for col in num_cols if col not in num_but_cat]\n",
    "    print(f\"Observations: {dataframe.shape[0]}\")\n",
    "    print(f\"Variables: {dataframe.shape[1]}\")\n",
    "    print(f'cat_cols: {len(cat_cols)}')\n",
    "    print(f'num_cols: {len(num_cols)}')\n",
    "    print(f'cat_but_car: {len(cat_but_car)}')\n",
    "    print(f'num_but_cat: {len(num_but_cat)}')\n",
    "    return cat_cols, cat_but_car, num_cols\n",
    "\n",
    "def cat_summary(dataframe, col_name, plot=False):\n",
    "    print(pd.DataFrame({col_name: dataframe[col_name].value_counts(),\n",
    "                        \"Ratio\": 100 * dataframe[col_name].value_counts() / len(dataframe)}))\n",
    "    if plot:\n",
    "        sns.countplot(x=dataframe[col_name], data=dataframe)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.savefig(f'{col_name}_countplot.png')\n",
    "        plt.close()\n",
    "\n",
    "def num_summary(dataframe, numerical_col, plot=False):\n",
    "    quantiles = [0.05, 0.10, 0.50, 0.90, 0.95, 0.99]\n",
    "    print(dataframe[numerical_col].describe(quantiles).T)\n",
    "    if plot:\n",
    "        dataframe[numerical_col].hist(bins=50)\n",
    "        plt.xlabel(numerical_col)\n",
    "        plt.title(numerical_col)\n",
    "        plt.savefig(f'{numerical_col}_histogram.png')\n",
    "        plt.close()\n",
    "    print(\"#####################################\")\n",
    "\n",
    "def target_summary_with_cat(dataframe, target, categorical_col):\n",
    "    print(pd.DataFrame({\"TARGET_MEAN\": dataframe.groupby(categorical_col)[target].mean()}), end=\"\\n\\n\\n\")\n",
    "\n",
    "def high_correlated_cols(dataframe, plot=False, corr_th=CORRELATION_THRESHOLD):\n",
    "    corr = dataframe.corr(numeric_only=True)\n",
    "    cor_matrix = corr.abs()\n",
    "    upper_triangle_matrix = cor_matrix.where(np.triu(np.ones(cor_matrix.shape), k=1).astype(bool))\n",
    "    drop_list = [col for col in upper_triangle_matrix.columns if any(upper_triangle_matrix[col] > corr_th)]\n",
    "    if plot:\n",
    "        sns.heatmap(corr, cmap=\"RdBu\", annot=True)\n",
    "        plt.savefig('correlation_heatmap.png')\n",
    "        plt.close()\n",
    "    return drop_list\n",
    "\n",
    "def outlier_thresholds(dataframe, variable, low_quantile=LOW_QUANTILE, up_quantile=UP_QUANTILE):\n",
    "    q1 = dataframe[variable].quantile(low_quantile)\n",
    "    q3 = dataframe[variable].quantile(up_quantile)\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "    return lower_bound, upper_bound\n",
    "\n",
    "def category_outlier_thresholds(df, col, category_col='Ã¼rÃ¼n kategorisi'):\n",
    "    thresholds = {}\n",
    "    for category in df[category_col].unique():\n",
    "        cat_df = df[df[category_col] == category]\n",
    "        low, up = outlier_thresholds(cat_df, col)\n",
    "        thresholds[category] = (low, up)\n",
    "    return thresholds\n",
    "\n",
    "def check_outlier(dataframe, col_name):\n",
    "    low_limit, up_limit = outlier_thresholds(dataframe, col_name)\n",
    "    return dataframe[(dataframe[col_name] > up_limit) | (dataframe[col_name] < low_limit)].any(axis=None)\n",
    "\n",
    "def replace_with_thresholds(dataframe, variable, category_col='Ã¼rÃ¼n kategorisi'):\n",
    "    thresholds = category_outlier_thresholds(dataframe, variable, category_col)\n",
    "    for category, (low, up) in thresholds.items():\n",
    "        dataframe.loc[(dataframe[category_col] == category) & (dataframe[variable] < low), variable] = low\n",
    "        dataframe.loc[(dataframe[category_col] == category) & (dataframe[variable] > up), variable] = up\n",
    "    return dataframe\n",
    "\n",
    "def missing_values_table(dataframe, na_name=False):\n",
    "    na_columns = [col for col in dataframe.columns if dataframe[col].isnull().sum() > 0]\n",
    "    n_miss = dataframe[na_columns].isnull().sum().sort_values(ascending=False)\n",
    "    ratio = (dataframe[na_columns].isnull().sum() / dataframe.shape[0] * 100).sort_values(ascending=False)\n",
    "    missing_df = pd.concat([n_miss, np.round(ratio, 2)], axis=1, keys=['n_miss', 'ratio'])\n",
    "    print(missing_df, end=\"\\n\")\n",
    "    if na_name:\n",
    "        return na_columns\n",
    "\n",
    "def remove_missing_values(dataframe):\n",
    "    print(\"##################### Missing Values Before #####################\")\n",
    "    print(dataframe.isnull().sum())\n",
    "    dataframe_cleaned = dataframe.dropna()\n",
    "    print(\"##################### Missing Values After #####################\")\n",
    "    print(dataframe_cleaned.isnull().sum())\n",
    "    return dataframe_cleaned\n",
    "\n",
    "def quick_missing_imp(data, num_method=NUM_METHOD, cat_length=CAT_LENGTH, target=\"Ã¼rÃ¼n fiyatÄ±\"):\n",
    "    variables_with_na = [col for col in data.columns if data[col].isnull().sum() > 0]\n",
    "    temp_target = data[target] if target in data.columns else None\n",
    "    print(\"# BEFORE\")\n",
    "    print(data[variables_with_na].isnull().sum(), \"\\n\")\n",
    "    data = data.apply(lambda x: x.fillna(x.mode()[0]) if (x.dtype == \"O\" and len(x.unique()) <= cat_length) else x, axis=0)\n",
    "    if num_method == \"mean\":\n",
    "        data = data.apply(lambda x: x.fillna(x.mean()) if x.dtype != \"O\" else x, axis=0)\n",
    "    elif num_method == \"median\":\n",
    "        data = data.apply(lambda x: x.fillna(x.median()) if x.dtype != \"O\" else x, axis=0)\n",
    "    if temp_target is not None:\n",
    "        data[target] = temp_target\n",
    "    print(\"# AFTER\")\n",
    "    print(\"Categorical variables filled with mode\")\n",
    "    print(f\"Numerical variables filled with {num_method}\")\n",
    "    print(data[variables_with_na].isnull().sum(), \"\\n\")\n",
    "    return data\n",
    "\n",
    "def rare_analyser(dataframe, target, cat_cols):\n",
    "    for col in cat_cols:\n",
    "        print(col, \":\", len(dataframe[col].value_counts()))\n",
    "        print(pd.DataFrame({\"COUNT\": dataframe[col].value_counts(),\n",
    "                            \"RATIO\": dataframe[col].value_counts() / len(dataframe),\n",
    "                            \"TARGET_MEAN\": dataframe.groupby(col)[target].mean()}), end=\"\\n\\n\\n\")\n",
    "\n",
    "def rare_encoder(dataframe, rare_perc):\n",
    "    temp_df = dataframe.copy()\n",
    "    rare_columns = [col for col in temp_df.columns if temp_df[col].dtype == 'O'\n",
    "                    and (temp_df[col].value_counts() / len(temp_df) < rare_perc).any(axis=None)]\n",
    "    for var in rare_columns:\n",
    "        tmp = temp_df[var].value_counts() / len(temp_df)\n",
    "        rare_labels = tmp[tmp < rare_perc].index\n",
    "        temp_df[var] = np.where(temp_df[var].isin(rare_labels), 'Rare', temp_df[var])\n",
    "    return temp_df\n",
    "\n",
    "def label_encoder(dataframe, binary_col):\n",
    "    labelencoder = LabelEncoder()\n",
    "    dataframe[binary_col] = labelencoder.fit_transform(dataframe[binary_col])\n",
    "    return dataframe\n",
    "\n",
    "def one_hot_encoder(dataframe, categorical_cols, drop_first=False):\n",
    "    return pd.get_dummies(dataframe, columns=categorical_cols, drop_first=drop_first)\n",
    "\n",
    "# DataLoader Class\n",
    "class DataLoader:\n",
    "    def __init__(self, training_data_path, testing_data_path):\n",
    "        self.training_data_path = training_data_path\n",
    "        self.testing_data_path = testing_data_path\n",
    "        print(\"Initializing DataLoader...\")\n",
    "\n",
    "    def get_data(self):\n",
    "        print(\"Loading data...\")\n",
    "        train = pd.read_csv(self.training_data_path)\n",
    "        test = pd.read_csv(self.testing_data_path)\n",
    "        df = pd.concat([train, test], ignore_index=True)\n",
    "        df = df.reset_index(drop=True)\n",
    "        print(\"Data loaded successfully.\")\n",
    "        return df\n",
    "\n",
    "# DataPreprocessing Class\n",
    "class DataPreprocessing:\n",
    "    def __init__(self, dataframe):\n",
    "        self.df = dataframe.copy()\n",
    "\n",
    "    def preprocess(self, is_test_only=False):\n",
    "        self.handle_outliers()\n",
    "        self.handle_missing_values()\n",
    "        self.feature_engineering()\n",
    "        self.drop_unnecessary_columns()\n",
    "        self.encode_features()\n",
    "\n",
    "        if is_test_only:\n",
    "            test_data = self.df[self.df['Ã¼rÃ¼n fiyatÄ±'].isnull()].drop('Ã¼rÃ¼n fiyatÄ±', axis=1)\n",
    "            test_ids = test_data[\"id\"].copy()\n",
    "            test_data = test_data.drop(columns=['id'])\n",
    "            return test_data, test_ids\n",
    "        else:\n",
    "            train_data = self.df[self.df['Ã¼rÃ¼n fiyatÄ±'].notnull()]\n",
    "            train_data = train_data.drop(columns=['id'])\n",
    "            X = train_data.drop('Ã¼rÃ¼n fiyatÄ±', axis=1)\n",
    "            y = train_data['Ã¼rÃ¼n fiyatÄ±']\n",
    "            y_log = np.log1p(y)  # Log transformation for target\n",
    "            X_train, X_val, y_train, y_val = train_test_split(X, y_log, test_size=0.10, random_state=42)\n",
    "            return X_train, X_val, y_train, y_val, y  # Return original y for error analysis\n",
    "\n",
    "    def handle_outliers(self):\n",
    "        num_cols = self.df.select_dtypes(include=np.number).columns\n",
    "        num_cols = [col for col in num_cols if col != 'Ã¼rÃ¼n fiyatÄ±']\n",
    "        for col in num_cols:\n",
    "            if check_outlier(self.df, col):\n",
    "                self.df = replace_with_thresholds(self.df, col)\n",
    "\n",
    "    def handle_missing_values(self):\n",
    "        num_cols = self.df.select_dtypes(include=np.number).columns\n",
    "        num_cols = [col for col in num_cols if col != 'Ã¼rÃ¼n fiyatÄ±']\n",
    "        self.df[num_cols] = self.df.groupby('Ã¼rÃ¼n kategorisi')[num_cols].transform(lambda x: x.fillna(x.median()))\n",
    "        cat_cols = self.df.select_dtypes(include='object').columns\n",
    "        for col in cat_cols:\n",
    "            self.df[col] = self.df[col].fillna(self.df[col].mode()[0])\n",
    "\n",
    "    def feature_engineering(self):\n",
    "        # Time-based features\n",
    "        self.df['tarih'] = pd.to_datetime(self.df['tarih'])\n",
    "        self.df['ay'] = self.df['tarih'].dt.month\n",
    "        self.df['Ã§eyrek'] = self.df['tarih'].dt.quarter\n",
    "        self.df['haftanÄ±n_gÃ¼nÃ¼'] = self.df['tarih'].dt.weekday\n",
    "\n",
    "        # Product and category-based features\n",
    "        self.df['besin_deÄŸeri_log'] = np.log1p(self.df['Ã¼rÃ¼n besin deÄŸeri'])\n",
    "        self.df['kategori_ortalama_besin'] = self.df.groupby('Ã¼rÃ¼n kategorisi')['Ã¼rÃ¼n besin deÄŸeri'].transform('mean')\n",
    "        self.df['Ã¼rÃ¼n_ortalama_fiyat'] = self.df.groupby('Ã¼rÃ¼n')['Ã¼rÃ¼n fiyatÄ±'].transform('mean')\n",
    "        self.df['kategori_fiyat_std'] = self.df.groupby('Ã¼rÃ¼n kategorisi')['Ã¼rÃ¼n fiyatÄ±'].transform('std')\n",
    "        self.df['besin_deÄŸeri_kategori'] = pd.qcut(self.df['Ã¼rÃ¼n besin deÄŸeri'], q=3, labels=['dÃ¼ÅŸÃ¼k', 'orta', 'yÃ¼ksek'])\n",
    "        self.df['Ã¼rÃ¼n_freq'] = self.df['Ã¼rÃ¼n'].map(self.df['Ã¼rÃ¼n'].value_counts() / len(self.df))\n",
    "\n",
    "    def drop_unnecessary_columns(self):\n",
    "        columns_to_drop = ['Ã¼rÃ¼n Ã¼retim yeri', 'market', 'ÅŸehir']\n",
    "        self.df.drop(columns=[col for col in columns_to_drop if col in self.df.columns], inplace=True)\n",
    "\n",
    "    def encode_features(self):\n",
    "        cat_cols, cat_but_car, num_cols = grab_col_names(self.df)\n",
    "        binary_cols = [col for col in cat_cols if self.df[col].nunique() <= 3]\n",
    "        for col in binary_cols:\n",
    "            self.df = label_encoder(self.df, col)\n",
    "        high_cardinality_cols = cat_but_car + [col for col in cat_cols if col not in binary_cols]\n",
    "        for col in high_cardinality_cols:\n",
    "            if col in self.df.columns:\n",
    "                train_data = self.df[self.df['Ã¼rÃ¼n fiyatÄ±'].notnull()]\n",
    "                target_means = train_data.groupby(col)['Ã¼rÃ¼n fiyatÄ±'].mean()\n",
    "                self.df[col] = self.df[col].map(target_means).fillna(train_data['Ã¼rÃ¼n fiyatÄ±'].mean())\n",
    "        remaining_cat_cols = [col for col in cat_cols if col not in binary_cols and col not in high_cardinality_cols]\n",
    "        if remaining_cat_cols:\n",
    "            self.df = one_hot_encoder(self.df, remaining_cat_cols, drop_first=True)\n",
    "        remaining_object_cols = self.df.select_dtypes(include='object').columns.tolist()\n",
    "        if remaining_object_cols:\n",
    "            raise ValueError(f\"Categorical columns not fully encoded: {remaining_object_cols}\")\n",
    "\n",
    "# HyperTuner Class\n",
    "class HyperTuner:\n",
    "    def __init__(self):\n",
    "        self.param_grids = {\n",
    "     \"CatBoost\": {\n",
    "    'iterations': [100],  # Sabit kalabilir, zaten yeterli\n",
    "    'learning_rate': [0.01, 0.02],  # 0.01 iyi, belki biraz daha hÄ±zlÄ± eÄŸitim iÃ§in 0.02 de denenebilir\n",
    "    'depth': [6, 8],  # 8 baÅŸarÄ±lÄ±, 6 alternatif olarak yeterli\n",
    "    'l2_leaf_reg': [5, 7, 9],  # 7 iyi, biraz etrafÄ± denenebilir\n",
    "    'bagging_temperature': [0.5, 1],  # 1 iÅŸe yarÄ±yor, ama 0.5 ile genel performans kontrolÃ¼\n",
    "    'border_count': [64, 128],  # 64 iÅŸe yarÄ±yor, 128 ile karÅŸÄ±laÅŸtÄ±rma yapÄ±labilir\n",
    "    'grow_policy': ['Lossguide', 'Depthwise']  # 'Lossguide' iÅŸe yarÄ±yor, sadece bir alternatif bÄ±rak\n",
    "}\n",
    ",\n",
    "            \"XGBoost\": {\n",
    "                'n_estimators': randint(50, 200),\n",
    "                'learning_rate': loguniform(0.005, 0.2),\n",
    "                'max_depth': [3, 5, 7, 9],\n",
    "                'subsample': uniform(0.6, 0.4),\n",
    "                'colsample_bytree': uniform(0.6, 0.4)\n",
    "            },\n",
    "            \"LightGBM\": {\n",
    "                'n_estimators': randint(50, 200),\n",
    "                'learning_rate': loguniform(0.005, 0.2),\n",
    "                'num_leaves': randint(20, 50),\n",
    "                'max_depth': [3, 5, 7, -1],\n",
    "                'subsample': uniform(0.6, 0.4),\n",
    "                'colsample_bytree': uniform(0.6, 0.4)\n",
    "            }\n",
    "        }\n",
    "        self.models = {\n",
    "            \"CatBoost\": CatBoostRegressor(silent=True, random_state=17),\n",
    "            \"XGBoost\": XGBRegressor(objective='reg:squarederror', random_state=17),\n",
    "            \"LightGBM\": LGBMRegressor(random_state=17),\n",
    "        }\n",
    "\n",
    "    def tune_model(self, model_name, X, y):\n",
    "        if model_name not in self.models:\n",
    "            raise ValueError(f\"Model {model_name} not found in models list.\")\n",
    "        model = self.models[model_name]\n",
    "        param_grid = self.param_grids[model_name]\n",
    "        search = RandomizedSearchCV(\n",
    "            estimator=model,\n",
    "            param_distributions=param_grid,\n",
    "            n_iter=10,\n",
    "            cv=3,\n",
    "            scoring='neg_mean_absolute_error',\n",
    "            n_jobs=-1,\n",
    "            random_state=42\n",
    "        )\n",
    "        search.fit(X, y)\n",
    "        return search.best_estimator_, search.best_params_\n",
    "\n",
    "# ModelEvaluator Class\n",
    "class ModelEvaluator:\n",
    "    def __init__(self, output_dir=\"predictions\"):\n",
    "        self.model_names = [\n",
    "            \"CatBoost\",\n",
    "        ]\n",
    "        self.best_model_name = \"CatBoost\"\n",
    "        self.rmse_scores = {}\n",
    "        self.mae_scores = {}\n",
    "        self.best_params = {}\n",
    "        self.tuner = HyperTuner()\n",
    "        self.trained_models = {}\n",
    "        self.output_dir = output_dir\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "\n",
    "    def evaluate_models(self, X, y, X_val, y_val, y_val_original):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=17)\n",
    "        print(\"Evaluating models with hyperparameter tuning...\")\n",
    "        for name in self.model_names:\n",
    "            best_model, best_params = self.tuner.tune_model(name, X_train, y_train)\n",
    "            self.trained_models[name] = best_model\n",
    "            self.best_params[name] = best_params\n",
    "            rmse = np.mean(np.sqrt(-cross_val_score(best_model, X, y, cv=5, scoring=\"neg_mean_squared_error\")))\n",
    "            self.rmse_scores[name] = rmse\n",
    "            mae = np.mean(-cross_val_score(best_model, X, y, cv=5, scoring=\"neg_mean_absolute_error\"))\n",
    "            self.mae_scores[name] = mae\n",
    "            print(f\"RMSE: {round(rmse, 4)} | MAE: {round(mae, 4)} ({name})\")\n",
    "            if self.best_params[name]:\n",
    "                print(f\"Best parameters: {self.best_params[name]}\")\n",
    "            # Category-based error analysis\n",
    "            y_pred_val = np.expm1(best_model.predict(X_val))\n",
    "            val_data = X_val.copy()\n",
    "            val_data['y_true'] = y_val_original\n",
    "            val_data['y_pred'] = y_pred_val\n",
    "            print(f\"\\nCategory-based MAE for {name}:\")\n",
    "            print(val_data.groupby('Ã¼rÃ¼n kategorisi').apply(lambda x: mean_absolute_error(x['y_true'], x['y_pred'])))\n",
    "\n",
    "        # Stacking Ensemble\n",
    "        print(\"\\nTraining Stacking Ensemble...\")\n",
    "        estimators = [(name, self.trained_models[name]) for name in self.model_names]\n",
    "        stacking_model = StackingRegressor(estimators=estimators, final_estimator=LinearRegression())\n",
    "        stacking_model.fit(X_train, y_train)\n",
    "        self.trained_models['Stacking'] = stacking_model\n",
    "        rmse = np.mean(np.sqrt(-cross_val_score(stacking_model, X, y, cv=5, scoring=\"neg_mean_squared_error\")))\n",
    "        self.rmse_scores['Stacking'] = rmse\n",
    "        mae = np.mean(-cross_val_score(stacking_model, X, y, cv=5, scoring=\"neg_mean_absolute_error\"))\n",
    "        self.mae_scores['Stacking'] = mae\n",
    "        print(f\"RMSE: {round(rmse, 4)} | MAE: {round(mae, 4)} (Stacking)\")\n",
    "\n",
    "        best_model_mae = min(self.mae_scores, key=self.mae_scores.get)\n",
    "        print(f\"\\nBest model based on MAE: {best_model_mae} (MAE: {round(self.mae_scores[best_model_mae], 4)})\")\n",
    "        best_model_rmse = min(self.rmse_scores, key=self.rmse_scores.get)\n",
    "        print(f\"Best model based on RMSE: {best_model_rmse} (RMSE: {round(self.rmse_scores[best_model_rmse], 4)})\")\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "    def train_and_predict(self, X_train, y_train, X_test, test_ids, output_file=\"submission.csv\"):\n",
    "        print(f\"\\nâ³ Training {self.best_model_name} model...\")\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "        best_model, best_params = self.tuner.tune_model(self.best_model_name, X_train, y_train)\n",
    "        self.best_params[self.best_model_name] = best_params\n",
    "        best_model.fit(X_train, y_train)\n",
    "        training_time = time.time() - start_time\n",
    "        print(f\"âœ… Training completed in {training_time:.2f} seconds\")\n",
    "        print(f\"ðŸ† Best parameters: {best_params}\")\n",
    "        predictions = np.expm1(best_model.predict(X_test))  # Inverse log transformation\n",
    "        submission_df = pd.DataFrame({\n",
    "            \"id\": test_ids.astype(int),\n",
    "            \"Ã¼rÃ¼n fiyatÄ±\": predictions.astype(float)\n",
    "        })\n",
    "        output_path = os.path.join(self.output_dir, output_file)\n",
    "        submission_df.to_csv(output_path, index=False, float_format='%.4f')\n",
    "        print(f\"\\nðŸ“ Predictions saved to '{output_path}'\")\n",
    "        print(f\"Sample predictions:\\n{submission_df.head()}\")\n",
    "        return predictions\n",
    "\n",
    "    def get_rmse_scores(self):\n",
    "        return self.rmse_scores\n",
    "\n",
    "    def get_mae_scores(self):\n",
    "        return self.mae_scores\n",
    "\n",
    "    def get_best_params(self):\n",
    "        return self.best_params\n",
    "\n",
    "# Main Function\n",
    "def main():\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    print(\"ðŸš€ Starting product price prediction pipeline...\")\n",
    "    print(\"\\nðŸ“‚ Loading data...\")\n",
    "    data_loader = DataLoader(TRAINING_DATA_PATH, TESTING_DATA_PATH)\n",
    "    combined_df = data_loader.get_data()\n",
    "    print(f\"âœ… Data loaded. Shape: {combined_df.shape}\")\n",
    "    print(\"\\nðŸ”§ Preprocessing data...\")\n",
    "    preprocessor = DataPreprocessing(combined_df)\n",
    "    X_train, X_val, y_train, y_val, y_val_original = preprocessor.preprocess()\n",
    "    print(f\"âœ… Training data prepared. Features: {X_train.shape[1]}, Samples: {X_train.shape[0]}\")\n",
    "    X_test_submission, test_ids = preprocessor.preprocess(is_test_only=True)\n",
    "    print(f\"âœ… Test data prepared. Samples: {X_test_submission.shape[0]}\")\n",
    "    print(\"\\nðŸ§ª Evaluating models...\")\n",
    "    evaluator = ModelEvaluator(output_dir=\"predictions/Boosting\")\n",
    "    evaluator.evaluate_models(X_train, y_train, X_val, y_val, y_val_original)\n",
    "    print(\"\\nðŸ”® Making final predictions...\")\n",
    "    predictions = evaluator.train_and_predict(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_test_submission,\n",
    "        test_ids,\n",
    "        output_file=\"submission.csv\"\n",
    "    )\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nðŸŽ‰ Pipeline completed in {total_time:.2f} seconds!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting product price prediction pipeline...\n",
      "\n",
      "ðŸ“‚ Loading data...\n",
      "Initializing DataLoader...\n",
      "Loading data...\n",
      "Data loaded successfully.\n",
      "âœ… Data loaded. Shape: (273024, 9)\n",
      "\n",
      "ðŸ”§ Preprocessing data...\n",
      "Observations: 273024\n",
      "Variables: 15\n",
      "cat_cols: 4\n",
      "num_cols: 10\n",
      "cat_but_car: 1\n",
      "num_but_cat: 3\n",
      "âœ… Training data prepared. Features: 13, Samples: 204768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19324/1562452927.py:118: FutureWarning: 'any' with datetime64 dtypes is deprecated and will raise in a future version. Use (obj != pd.Timestamp(0)).any() instead.\n",
      "  return dataframe[(dataframe[col_name] > up_limit) | (dataframe[col_name] < low_limit)].any(axis=None)\n",
      "/tmp/ipykernel_19324/1562452927.py:118: FutureWarning: 'any' with datetime64 dtypes is deprecated and will raise in a future version. Use (obj != pd.Timestamp(0)).any() instead.\n",
      "  return dataframe[(dataframe[col_name] > up_limit) | (dataframe[col_name] < low_limit)].any(axis=None)\n",
      "/tmp/ipykernel_19324/1562452927.py:118: FutureWarning: 'any' with datetime64 dtypes is deprecated and will raise in a future version. Use (obj != pd.Timestamp(0)).any() instead.\n",
      "  return dataframe[(dataframe[col_name] > up_limit) | (dataframe[col_name] < low_limit)].any(axis=None)\n",
      "/tmp/ipykernel_19324/1562452927.py:118: FutureWarning: 'any' with datetime64 dtypes is deprecated and will raise in a future version. Use (obj != pd.Timestamp(0)).any() instead.\n",
      "  return dataframe[(dataframe[col_name] > up_limit) | (dataframe[col_name] < low_limit)].any(axis=None)\n",
      "/tmp/ipykernel_19324/1562452927.py:118: FutureWarning: 'any' with datetime64 dtypes is deprecated and will raise in a future version. Use (obj != pd.Timestamp(0)).any() instead.\n",
      "  return dataframe[(dataframe[col_name] > up_limit) | (dataframe[col_name] < low_limit)].any(axis=None)\n",
      "/tmp/ipykernel_19324/1562452927.py:118: FutureWarning: 'any' with datetime64 dtypes is deprecated and will raise in a future version. Use (obj != pd.Timestamp(0)).any() instead.\n",
      "  return dataframe[(dataframe[col_name] > up_limit) | (dataframe[col_name] < low_limit)].any(axis=None)\n",
      "/tmp/ipykernel_19324/1562452927.py:118: FutureWarning: 'any' with datetime64 dtypes is deprecated and will raise in a future version. Use (obj != pd.Timestamp(0)).any() instead.\n",
      "  return dataframe[(dataframe[col_name] > up_limit) | (dataframe[col_name] < low_limit)].any(axis=None)\n",
      "/tmp/ipykernel_19324/1562452927.py:118: FutureWarning: 'any' with datetime64 dtypes is deprecated and will raise in a future version. Use (obj != pd.Timestamp(0)).any() instead.\n",
      "  return dataframe[(dataframe[col_name] > up_limit) | (dataframe[col_name] < low_limit)].any(axis=None)\n",
      "/tmp/ipykernel_19324/1562452927.py:118: FutureWarning: 'any' with datetime64 dtypes is deprecated and will raise in a future version. Use (obj != pd.Timestamp(0)).any() instead.\n",
      "  return dataframe[(dataframe[col_name] > up_limit) | (dataframe[col_name] < low_limit)].any(axis=None)\n",
      "/tmp/ipykernel_19324/1562452927.py:118: FutureWarning: 'any' with datetime64 dtypes is deprecated and will raise in a future version. Use (obj != pd.Timestamp(0)).any() instead.\n",
      "  return dataframe[(dataframe[col_name] > up_limit) | (dataframe[col_name] < low_limit)].any(axis=None)\n",
      "/tmp/ipykernel_19324/1562452927.py:118: FutureWarning: 'any' with datetime64 dtypes is deprecated and will raise in a future version. Use (obj != pd.Timestamp(0)).any() instead.\n",
      "  return dataframe[(dataframe[col_name] > up_limit) | (dataframe[col_name] < low_limit)].any(axis=None)\n",
      "/tmp/ipykernel_19324/1562452927.py:118: FutureWarning: 'any' with datetime64 dtypes is deprecated and will raise in a future version. Use (obj != pd.Timestamp(0)).any() instead.\n",
      "  return dataframe[(dataframe[col_name] > up_limit) | (dataframe[col_name] < low_limit)].any(axis=None)\n",
      "/tmp/ipykernel_19324/1562452927.py:118: FutureWarning: 'any' with datetime64 dtypes is deprecated and will raise in a future version. Use (obj != pd.Timestamp(0)).any() instead.\n",
      "  return dataframe[(dataframe[col_name] > up_limit) | (dataframe[col_name] < low_limit)].any(axis=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations: 273024\n",
      "Variables: 15\n",
      "cat_cols: 3\n",
      "num_cols: 12\n",
      "cat_but_car: 0\n",
      "num_but_cat: 3\n",
      "âœ… Test data prepared. Samples: 45504\n",
      "\n",
      "ðŸ§ª Evaluating models...\n",
      "Evaluating models with hyperparameter tuning...\n",
      "RMSE: 0.2893 | MAE: 0.2224 (CatBoost)\n",
      "Best parameters: {'learning_rate': 0.01, 'l2_leaf_reg': 7, 'iterations': 100, 'grow_policy': 'Lossguide', 'depth': 8, 'border_count': 64, 'bagging_temperature': 1}\n",
      "\n",
      "Category-based MAE for CatBoost:\n",
      "Ã¼rÃ¼n kategorisi\n",
      "9.976630      2.555764\n",
      "10.420985     1.792725\n",
      "15.534468     3.423504\n",
      "26.588236     7.292074\n",
      "31.349666     8.982162\n",
      "36.961375    15.158272\n",
      "dtype: float64\n",
      "\n",
      "Training Stacking Ensemble...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19324/1562452927.py:368: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  print(val_data.groupby('Ã¼rÃ¼n kategorisi').apply(lambda x: mean_absolute_error(x['y_true'], x['y_pred'])))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.0856 | MAE: 0.0647 (Stacking)\n",
      "\n",
      "Best model based on MAE: Stacking (MAE: 0.0647)\n",
      "Best model based on RMSE: Stacking (RMSE: 0.0856)\n",
      "\n",
      "ðŸ”® Making final predictions...\n",
      "\n",
      "â³ Training CatBoost model...\n",
      "âœ… Training completed in 67.37 seconds\n",
      "ðŸ† Best parameters: {'learning_rate': 0.01, 'l2_leaf_reg': 7, 'iterations': 100, 'grow_policy': 'Lossguide', 'depth': 8, 'border_count': 64, 'bagging_temperature': 1}\n",
      "\n",
      "ðŸ“ Predictions saved to 'predictions/Boosting/submission.csv'\n",
      "Sample predictions:\n",
      "        id  Ã¼rÃ¼n fiyatÄ±\n",
      "227520   0    39.523662\n",
      "227521   1    23.880532\n",
      "227522   2    25.886491\n",
      "227523   3    19.666484\n",
      "227524   4    29.171648\n",
      "\n",
      "ðŸŽ‰ Pipeline completed in 267.67 seconds!\n"
     ]
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
