{
 "cells": [
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-05-05T08:51:17.560750Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configuration\n",
    "DATASET_PATH = 'dataset/'  # Adjust for Kaggle dataset path\n",
    "TRAINING_DATA_PATH = DATASET_PATH + \"train.csv\"\n",
    "TESTING_DATA_PATH = DATASET_PATH + \"testFeatures.csv\"\n",
    "LOW_QUANTILE = 0.1\n",
    "UP_QUANTILE = 0.9\n",
    "CAT_THRESHOLD = 10\n",
    "CAR_THRESHOLD = 10\n",
    "CAT_LENGTH = 30\n",
    "NUM_METHOD = \"median\"\n",
    "\n",
    "# Helper Functions\n",
    "def check_df(dataframe):\n",
    "    print(\"##################### Shape #####################\")\n",
    "    print(dataframe.shape)\n",
    "    print(\"##################### Types #####################\")\n",
    "    print(dataframe.dtypes)\n",
    "    print(\"##################### Columns #####################\")\n",
    "    print(dataframe.columns.tolist())\n",
    "    print(\"##################### Head #####################\")\n",
    "    print(dataframe.head(3))\n",
    "    print(\"##################### Tail #####################\")\n",
    "    print(dataframe.tail(3))\n",
    "    print(\"##################### NA #####################\")\n",
    "    print(dataframe.isnull().sum())\n",
    "    print(\"##################### Quantiles #####################\")\n",
    "    print(dataframe.quantile([0, 0.05, 0.50, 0.95, 0.99, 1]).T)\n",
    "\n",
    "def grab_col_names(dataframe, cat_th=CAT_THRESHOLD, car_th=CAR_THRESHOLD):\n",
    "    cat_cols = [col for col in dataframe.columns if dataframe[col].dtype == \"O\"]\n",
    "    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() < cat_th and dataframe[col].dtype != \"O\"]\n",
    "    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() > car_th and dataframe[col].dtype == \"O\"]\n",
    "    cat_cols = cat_cols + num_but_cat\n",
    "    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n",
    "    num_cols = [col for col in dataframe.columns if dataframe[col].dtype != \"O\"]\n",
    "    num_cols = [col for col in num_cols if col not in num_but_cat]\n",
    "    print(f\"Observations: {dataframe.shape[0]}\")\n",
    "    print(f\"Variables: {dataframe.shape[1]}\")\n",
    "    print(f'cat_cols: {len(cat_cols)}')\n",
    "    print(f'num_cols: {len(num_cols)}')\n",
    "    print(f'cat_but_car: {len(cat_but_car)}')\n",
    "    print(f'num_but_cat: {len(num_but_cat)}')\n",
    "    return cat_cols, cat_but_car, num_cols\n",
    "\n",
    "def cat_summary(dataframe, col_name, plot=False):\n",
    "    print(pd.DataFrame({col_name: dataframe[col_name].value_counts(),\n",
    "                        \"Ratio\": 100 * dataframe[col_name].value_counts() / len(dataframe)}))\n",
    "    if plot:\n",
    "        sns.countplot(x=dataframe[col_name], data=dataframe)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.savefig(f'{col_name}_countplot.png')\n",
    "        plt.close()\n",
    "\n",
    "def num_summary(dataframe, numerical_col, plot=False):\n",
    "    quantiles = [0.05, 0.10, 0.50, 0.90, 0.95, 0.99]\n",
    "    print(dataframe[numerical_col].describe(quantiles).T)\n",
    "    if plot:\n",
    "        dataframe[numerical_col].hist(bins=50)\n",
    "        plt.xlabel(numerical_col)\n",
    "        plt.title(numerical_col)\n",
    "        plt.savefig(f'{numerical_col}_histogram.png')\n",
    "        plt.close()\n",
    "    print(\"########10#############################\")\n",
    "\n",
    "def target_summary_with_cat(dataframe, target, categorical_col):\n",
    "    print(pd.DataFrame({\"TARGET_MEAN\": dataframe.groupby(categorical_col)[target].mean()}), end=\"\\n\\n\\n\")\n",
    "\n",
    "def outlier_thresholds(dataframe, variable, low_quantile=LOW_QUANTILE, up_quantile=UP_QUANTILE):\n",
    "    q1 = dataframe[variable].quantile(low_quantile)\n",
    "    q3 = dataframe[variable].quantile(up_quantile)\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "    return lower_bound, upper_bound\n",
    "\n",
    "def category_outlier_thresholds(df, col, category_col='Ã¼rÃ¼n kategorisi'):\n",
    "    thresholds = {}\n",
    "    for category in df[category_col].unique():\n",
    "        cat_df = df[df[category_col] == category]\n",
    "        low, up = outlier_thresholds(cat_df, col)\n",
    "        thresholds[category] = (low, up)\n",
    "    return thresholds\n",
    "\n",
    "def check_outlier(dataframe, col_name):\n",
    "    if dataframe[col_name].dtype in ['datetime64[ns]', 'datetime64']:\n",
    "        return False\n",
    "    low_limit, up_limit = outlier_thresholds(dataframe, col_name)\n",
    "    return dataframe[(dataframe[col_name] > up_limit) | (dataframe[col_name] < low_limit)].any(axis=None)\n",
    "\n",
    "def replace_with_thresholds(dataframe, variable, category_col='Ã¼rÃ¼n kategorisi'):\n",
    "    thresholds = category_outlier_thresholds(dataframe, variable, category_col)\n",
    "    for category, (low, up) in thresholds.items():\n",
    "        dataframe.loc[(dataframe[category_col] == category) & (dataframe[variable] < low), variable] = low\n",
    "        dataframe.loc[(dataframe[category_col] == category) & (dataframe[variable] > up), variable] = up\n",
    "    return dataframe\n",
    "\n",
    "def missing_values_table(dataframe, na_name=False):\n",
    "    na_columns = [col for col in dataframe.columns if dataframe[col].isnull().sum() > 0]\n",
    "    n_miss = dataframe[na_columns].isnull().sum().sort_values(ascending=False)\n",
    "    ratio = (dataframe[na_columns].isnull().sum() / dataframe.shape[0] * 100).sort_values(ascending=False)\n",
    "    missing_df = pd.concat([n_miss, np.round(ratio, 2)], axis=1, keys=['n_miss', 'ratio'])\n",
    "    print(missing_df, end=\"\\n\")\n",
    "    if na_name:\n",
    "        return na_columns\n",
    "\n",
    "def quick_missing_imp(data, num_method=NUM_METHOD, cat_length=CAT_LENGTH, target=\"Ã¼rÃ¼n fiyatÄ±\"):\n",
    "    variables_with_na = [col for col in data.columns if data[col].isnull().sum() > 0]\n",
    "    temp_target = data[target] if target in data.columns else None\n",
    "    print(\"# BEFORE\")\n",
    "    print(data[variables_with_na].isnull().sum(), \"\\n\")\n",
    "    data = data.apply(lambda x: x.fillna(x.mode()[0]) if (x.dtype == \"O\" and len(x.unique()) <= cat_length) else x, axis=0)\n",
    "    if num_method == \"mean\":\n",
    "        data = data.apply(lambda x: x.fillna(x.mean()) if x.dtype != \"O\" else x, axis=0)\n",
    "    elif num_method == \"median\":\n",
    "        data = data.apply(lambda x: x.fillna(x.median()) if x.dtype != \"O\" else x, axis=0)\n",
    "    if temp_target is not None:\n",
    "        data[target] = temp_target\n",
    "    print(\"# AFTER\")\n",
    "    print(\"Categorical variables filled with mode\")\n",
    "    print(f\"Numerical variables filled with {num_method}\")\n",
    "    print(data[variables_with_na].isnull().sum(), \"\\n\")\n",
    "    return data\n",
    "\n",
    "def label_encoder(dataframe, binary_col):\n",
    "    labelencoder = LabelEncoder()\n",
    "    dataframe[binary_col] = labelencoder.fit_transform(dataframe[binary_col])\n",
    "    return dataframe\n",
    "\n",
    "def one_hot_encoder(dataframe, categorical_cols, drop_first=False):\n",
    "    return pd.get_dummies(dataframe, columns=categorical_cols, drop_first=drop_first)\n",
    "\n",
    "# DataLoader Class\n",
    "class DataLoader:\n",
    "    def __init__(self, training_data_path, testing_data_path):\n",
    "        self.training_data_path = training_data_path\n",
    "        self.testing_data_path = testing_data_path\n",
    "        print(\"Initializing DataLoader...\")\n",
    "\n",
    "    def get_data(self):\n",
    "        print(\"Loading data...\")\n",
    "        train = pd.read_csv(self.training_data_path)\n",
    "        test = pd.read_csv(self.testing_data_path)\n",
    "        df = pd.concat([train, test], ignore_index=True)\n",
    "        df = df.reset_index(drop=True)\n",
    "        print(\"Data loaded successfully.\")\n",
    "        return df\n",
    "\n",
    "# DataPreprocessing Class\n",
    "class DataPreprocessing:\n",
    "    def __init__(self, dataframe):\n",
    "        self.df = dataframe.copy()\n",
    "\n",
    "    def preprocess(self, is_test_only=False):\n",
    "        self.handle_outliers()\n",
    "        self.handle_missing_values()\n",
    "        self.feature_engineering()\n",
    "        self.drop_unnecessary_columns()\n",
    "        self.encode_features()\n",
    "\n",
    "        if not is_test_only:\n",
    "            train_data = self.df[self.df['Ã¼rÃ¼n fiyatÄ±'].notnull()]\n",
    "            train_data = train_data.drop(columns=['id'])\n",
    "            X = train_data.drop('Ã¼rÃ¼n fiyatÄ±', axis=1)\n",
    "            y = train_data['Ã¼rÃ¼n fiyatÄ±']\n",
    "            y_log = np.log1p(y)\n",
    "            X_train, X_val, y_train, y_val = train_test_split(X, y_log, test_size=0.10, random_state=42)\n",
    "            non_numeric_cols = X_train.select_dtypes(exclude=['int64', 'float64', 'int32']).columns\n",
    "            if non_numeric_cols.any():\n",
    "                raise ValueError(f\"Non-numeric columns found in X_train: {non_numeric_cols}\")\n",
    "            return X_train, X_val, y_train, y_val, y\n",
    "        else:\n",
    "            test_data = self.df[self.df['Ã¼rÃ¼n fiyatÄ±'].isnull()].drop('Ã¼rÃ¼n fiyatÄ±', axis=1)\n",
    "            test_ids = test_data[\"id\"].copy()\n",
    "            test_data = test_data.drop(columns=['id'])\n",
    "            non_numeric_cols = test_data.select_dtypes(exclude=['int64', 'float64', 'int32']).columns\n",
    "            if non_numeric_cols.any():\n",
    "                raise ValueError(f\"Non-numeric columns found in X_test: {non_numeric_cols}\")\n",
    "            return test_data, test_ids\n",
    "\n",
    "    def handle_outliers(self):\n",
    "        num_cols = self.df.select_dtypes(include=np.number).columns\n",
    "        num_cols = [col for col in num_cols if col != 'Ã¼rÃ¼n fiyatÄ±']\n",
    "        for col in num_cols:\n",
    "            if check_outlier(self.df, col):\n",
    "                self.df = replace_with_thresholds(self.df, col)\n",
    "\n",
    "    def handle_missing_values(self):\n",
    "        num_cols = self.df.select_dtypes(include=np.number).columns\n",
    "        num_cols = [col for col in num_cols if col != 'Ã¼rÃ¼n fiyatÄ±']\n",
    "        self.df[num_cols] = self.df.groupby('Ã¼rÃ¼n kategorisi')[num_cols].transform(lambda x: x.fillna(x.median()))\n",
    "        cat_cols = self.df.select_dtypes(include='object').columns\n",
    "        for col in cat_cols:\n",
    "            self.df[col] = self.df[col].fillna(self.df[col].mode()[0])\n",
    "\n",
    "    def feature_engineering(self):\n",
    "        date_col = None\n",
    "        possible_date_cols = ['tarih', 'Tarih', 'date', 'Date', 'tarih_yil', 'timestamp']\n",
    "        for col in possible_date_cols:\n",
    "            if col in self.df.columns:\n",
    "                date_col = col\n",
    "                break\n",
    "\n",
    "        if date_col:\n",
    "            print(f\"Found date column: {date_col}\")\n",
    "            self.df[date_col] = pd.to_datetime(self.df[date_col])\n",
    "            self.df['ay'] = self.df[date_col].dt.month\n",
    "            self.df['Ã§eyrek'] = self.df[date_col].dt.quarter\n",
    "            self.df['haftanÄ±n_gÃ¼nÃ¼'] = self.df[date_col].dt.weekday\n",
    "        else:\n",
    "            print(\"No date column found. Skipping time-based features.\")\n",
    "            self.df['ay'] = 0\n",
    "            self.df['Ã§eyrek'] = 0\n",
    "            self.df['haftanÄ±n_gÃ¼nÃ¼'] = 0\n",
    "\n",
    "        self.df['besin_deÄŸeri_log'] = np.log1p(self.df['Ã¼rÃ¼n besin deÄŸeri'])\n",
    "        self.df['kategori_ortalama_besin'] = self.df.groupby('Ã¼rÃ¼n kategorisi')['Ã¼rÃ¼n besin deÄŸeri'].transform('mean')\n",
    "        self.df['Ã¼rÃ¼n_ortalama_fiyat'] = self.df.groupby('Ã¼rÃ¼n')['Ã¼rÃ¼n fiyatÄ±'].transform('mean')\n",
    "        self.df['kategori_fiyat_std'] = self.df.groupby('Ã¼rÃ¼n kategorisi')['Ã¼rÃ¼n fiyatÄ±'].transform('std')\n",
    "        self.df['besin_deÄŸeri_kategori'] = pd.qcut(self.df['Ã¼rÃ¼n besin deÄŸeri'], q=3, labels=['dÃ¼ÅŸÃ¼k', 'orta', 'yÃ¼ksek'])\n",
    "        self.df['Ã¼rÃ¼n_freq'] = self.df['Ã¼rÃ¼n'].map(self.df['Ã¼rÃ¼n'].value_counts() / len(self.df))\n",
    "\n",
    "    def drop_unnecessary_columns(self):\n",
    "        columns_to_drop = ['Ã¼rÃ¼n Ã¼retim yeri', 'market', 'ÅŸehir'] + [col for col in ['tarih', 'Tarih', 'date', 'Date', 'tarih_yil', 'timestamp'] if col in self.df.columns]\n",
    "        self.df.drop(columns=[col for col in columns_to_drop if col in self.df.columns], inplace=True)\n",
    "\n",
    "    def encode_features(self):\n",
    "        cat_cols, cat_but_car, num_cols = grab_col_names(self.df)\n",
    "        binary_cols = [col for col in cat_cols if self.df[col].nunique() <= 3]\n",
    "        for col in binary_cols:\n",
    "            self.df = label_encoder(self.df, col)\n",
    "        high_cardinality_cols = cat_but_car + [col for col in cat_cols if col not in binary_cols]\n",
    "        for col in high_cardinality_cols:\n",
    "            if col in self.df.columns:\n",
    "                train_data = self.df[self.df['Ã¼rÃ¼n fiyatÄ±'].notnull()]\n",
    "                target_means = train_data.groupby(col)['Ã¼rÃ¼n fiyatÄ±'].mean()\n",
    "                self.df[col] = self.df[col].map(target_means).fillna(train_data['Ã¼rÃ¼n fiyatÄ±'].mean())\n",
    "        remaining_cat_cols = [col for col in cat_cols if col not in binary_cols and col not in high_cardinality_cols]\n",
    "        if remaining_cat_cols:\n",
    "            self.df = one_hot_encoder(self.df, remaining_cat_cols, drop_first=True)\n",
    "        remaining_object_cols = self.df.select_dtypes(include='object').columns.tolist()\n",
    "        if remaining_object_cols:\n",
    "            raise ValueError(f\"Categorical columns not fully encoded: {remaining_object_cols}\")\n",
    "\n",
    "# HyperTuner Class\n",
    "class HyperTuner:\n",
    "    def __init__(self):\n",
    "        self.param_grid = {\n",
    "         'n_estimators': randint(100, 600),  # Wider range for more robust ensembles\n",
    "    'max_depth': [None, 10, 20, 30, 40],  # Expanded depth options for flexibility\n",
    "    'min_samples_split': [2, 5, 10, 15],  # Broader range to control overfitting\n",
    "    'min_samples_leaf': [1, 2, 4, 8],  # More options to regularize leaf size\n",
    "    'max_features': ['sqrt', 'log2', 0.2, 0.3, 0.4, 0.5],  # Diverse feature sampling strategies\n",
    "    'bootstrap': [True, False],  # Test both bootstrapping and non-bootstrapping\n",
    "    'max_leaf_nodes': [None, 500, 1000, 2000],  # Control tree complexity\n",
    "    'min_impurity_decrease': uniform(0.0, 0.01),  # Regularize splits by impurity reduction\n",
    "    'ccp_alpha': uniform(0.0, 0.02)  # Cost-complexity pruning for simpler trees\n",
    "        }\n",
    "        self.model = RandomForestRegressor(random_state=17)\n",
    "\n",
    "    def tune_model(self, X, y):\n",
    "        search = RandomizedSearchCV(\n",
    "            estimator=self.model,\n",
    "            param_distributions=self.param_grid,\n",
    "            n_iter=50,  # Increased for better exploration\n",
    "            cv=8,\n",
    "            scoring='neg_mean_absolute_error',\n",
    "            n_jobs=-1,\n",
    "            random_state=42\n",
    "        )\n",
    "        search.fit(X, y)\n",
    "        return search.best_estimator_, search.best_params_\n",
    "\n",
    "# ModelEvaluator Class\n",
    "class ModelEvaluator:\n",
    "    def __init__(self, output_dir=\"predictions/RF/local\"):\n",
    "        self.model_name = \"RandomForest\"\n",
    "        self.rmse_score = None\n",
    "        self.mae_score = None\n",
    "        self.best_params = None\n",
    "        self.tuner = HyperTuner()\n",
    "        self.trained_model = None\n",
    "        self.output_dir = output_dir\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "\n",
    "    def evaluate_model(self, X, y, X_val, y_val, y_val_original):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=17)\n",
    "        print(\"Evaluating Random Forest with hyperparameter tuning...\")\n",
    "        best_model, best_params = self.tuner.tune_model(X_train, y_train)\n",
    "        self.trained_model = best_model\n",
    "        self.best_params = best_params\n",
    "        rmse = np.mean(np.sqrt(-cross_val_score(best_model, X, y, cv=5, scoring=\"neg_mean_squared_error\")))\n",
    "        self.rmse_score = rmse\n",
    "        mae = np.mean(-cross_val_score(best_model, X, y, cv=5, scoring=\"neg_mean_absolute_error\"))\n",
    "        self.mae_score = mae\n",
    "        print(f\"RMSE: {round(rmse, 4)} | MAE: {round(mae, 4)} ({self.model_name})\")\n",
    "        print(f\"Best parameters: {best_params}\")\n",
    "        y_pred_val = np.expm1(best_model.predict(X_val))\n",
    "        val_data = X_val.copy()\n",
    "        val_data['y_true'] = y_val_original\n",
    "        val_data['y_pred'] = y_pred_val\n",
    "        print(f\"\\nCategory-based MAE for {self.model_name}:\")\n",
    "        print(val_data.groupby('Ã¼rÃ¼n kategorisi').apply(lambda x: mean_absolute_error(x['y_true'], x['y_pred'])))\n",
    "        importances = best_model.feature_importances_\n",
    "        feature_importance = pd.DataFrame({'feature': X.columns, 'importance': importances}).sort_values('importance', ascending=False)\n",
    "        print(\"\\nFeature Importance:\")\n",
    "        print(feature_importance)\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "    def train_and_predict(self, X_train, y_train, X_test, test_ids, output_file=\"submission.csv\"):\n",
    "        print(f\"\\nâ³ Training {self.model_name} model...\")\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "        best_model, best_params = self.tuner.tune_model(X_train, y_train)\n",
    "        self.best_params = best_params\n",
    "        best_model.fit(X_train, y_train)\n",
    "        training_time = time.time() - start_time\n",
    "        print(f\"âœ… Training completed in {training_time:.2f} seconds\")\n",
    "        print(f\"ðŸ† Best parameters: {best_params}\")\n",
    "        predictions = np.expm1(best_model.predict(X_test))\n",
    "        submission_df = pd.DataFrame({\n",
    "            \"id\": test_ids.astype(int),\n",
    "            \"Ã¼rÃ¼n fiyatÄ±\": predictions.astype(float)\n",
    "        })\n",
    "        output_path = os.path.join(self.output_dir, output_file)\n",
    "        submission_df.to_csv(output_path, index=False, float_format='%.4f')\n",
    "        print(f\"\\nðŸ“ Predictions saved to '{output_path}'\")\n",
    "        print(f\"Sample predictions:\\n{submission_df.head()}\")\n",
    "        return predictions\n",
    "\n",
    "    def get_rmse_score(self):\n",
    "        return self.rmse_score\n",
    "\n",
    "    def get_mae_score(self):\n",
    "        return self.mae_score\n",
    "\n",
    "    def get_best_params(self):\n",
    "        return self.best_params\n",
    "\n",
    "# Main Function\n",
    "def main():\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    print(\"ðŸš€ Starting product price prediction pipeline with Random Forest...\")\n",
    "    print(\"\\nðŸ“‚ Loading data...\")\n",
    "    data_loader = DataLoader(TRAINING_DATA_PATH, TESTING_DATA_PATH)\n",
    "    combined_df = data_loader.get_data()\n",
    "    print(f\"âœ… Data loaded. Shape: {combined_df.shape}\")\n",
    "    print(\"\\nðŸ”§ Preprocessing data...\")\n",
    "    preprocessor = DataPreprocessing(combined_df)\n",
    "    X_train, X_val, y_train, y_val, y_val_original = preprocessor.preprocess()\n",
    "    print(f\"âœ… Training data prepared. Features: {X_train.shape[1]}, Samples: {X_train.shape[0]}\")\n",
    "    X_test_submission, test_ids = preprocessor.preprocess(is_test_only=True)\n",
    "    print(f\"âœ… Test data prepared. Samples: {X_test_submission.shape[0]}\")\n",
    "    print(\"\\nðŸ§ª Evaluating Random Forest model...\")\n",
    "    evaluator = ModelEvaluator(output_dir=\"predictions/RF/local\")\n",
    "    X_train, X_test, y_train, y_test = evaluator.evaluate_model(X_train, y_train, X_val, y_val, y_val_original)\n",
    "    print(\"\\nðŸ”® Making final predictions...\")\n",
    "    predictions = evaluator.train_and_predict(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_test_submission,\n",
    "        test_ids,\n",
    "        output_file=\"submission.csv\"\n",
    "    )\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nðŸŽ‰ Pipeline completed in {total_time:.2f} seconds!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "b9e551659cd52122",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting product price prediction pipeline with Random Forest...\n",
      "\n",
      "ðŸ“‚ Loading data...\n",
      "Initializing DataLoader...\n",
      "Loading data...\n",
      "Data loaded successfully.\n",
      "âœ… Data loaded. Shape: (273024, 9)\n",
      "\n",
      "ðŸ”§ Preprocessing data...\n",
      "Found date column: tarih\n",
      "Observations: 273024\n",
      "Variables: 14\n",
      "cat_cols: 7\n",
      "num_cols: 6\n",
      "cat_but_car: 1\n",
      "num_but_cat: 6\n",
      "âœ… Training data prepared. Features: 12, Samples: 204768\n",
      "No date column found. Skipping time-based features.\n",
      "Observations: 273024\n",
      "Variables: 14\n",
      "cat_cols: 9\n",
      "num_cols: 5\n",
      "cat_but_car: 0\n",
      "num_but_cat: 9\n",
      "âœ… Test data prepared. Samples: 45504\n",
      "\n",
      "ðŸ§ª Evaluating Random Forest model...\n",
      "Evaluating Random Forest with hyperparameter tuning...\n"
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
