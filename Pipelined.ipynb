{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 99039,
     "databundleVersionId": 11820474,
     "sourceType": "competition"
    }
   ],
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": "# %% [markdown]\n\"\"\"\n# Product Price Prediction Project - Jupyter Notebook Version\n\nThis notebook combines all the Python files from your project into a single interactive notebook with the same functionality.\n\"\"\"\n\n# %%\n# Import all required libraries\nimport numpy as np\nimport pandas as pd\nimport os\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV, RandomizedSearchCV\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.preprocessing import LabelEncoder\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nfrom scipy.stats import uniform, randint, loguniform\n\n# Set random seed for reproducibility\nnp.random.seed(42)",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T06:32:18.129637967Z",
     "start_time": "2025-05-05T06:15:27.911327Z"
    }
   },
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "source": [
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## Configuration\n",
    "\"\"\"\n",
    "# %%\n",
    "# Configuration (equivalent to config.py)\n",
    "DATASET_PATH = 'dataset/'\n",
    "TRAINING_DATA_PATH = DATASET_PATH + \"train.csv\"\n",
    "TESTING_DATA_PATH = DATASET_PATH + \"testFeatures.csv\"\n",
    "\n",
    "LOW_QUANTILE = 0.02\n",
    "UP_QUANTILE = 0.98\n",
    "CAT_THRESHOLD = 5\n",
    "CAR_THRESHOLD = 20\n",
    "CORRELATION_THRESHOLD = 0.30\n",
    "CAT_LENGTH = 20\n",
    "NUM_METHOD = \"median\""
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T06:32:18.130641761Z",
     "start_time": "2025-05-05T06:15:27.968431Z"
    }
   },
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "source": "# %% [markdown]\n\"\"\"\n## Helper Functions\n\"\"\"\n# %%\n# Helper functions (equivalent to helpers.py)\ndef check_df(dataframe):\n    print(\"##################### Shape #####################\")\n    print(dataframe.shape)\n    print(\"##################### Types #####################\")\n    print(dataframe.dtypes)\n    print(\"##################### Head #####################\")\n    print(dataframe.head(3))\n    print(\"##################### Tail #####################\")\n    print(dataframe.tail(3))\n    print(\"##################### NA #####################\")\n    print(dataframe.isnull().sum())\n    print(\"##################### Quantiles #####################\")\n    print(dataframe.quantile([0, 0.05, 0.50, 0.95, 0.99, 1]).T)\n\ndef grab_col_names(dataframe, cat_th=CAT_THRESHOLD, car_th=CAR_THRESHOLD):\n    cat_cols = [col for col in dataframe.columns if dataframe[col].dtype == \"O\"]\n    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() < cat_th and dataframe[col].dtype != \"O\"]\n    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() > car_th and dataframe[col].dtype == \"O\"]\n    cat_cols = cat_cols + num_but_cat\n    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n    num_cols = [col for col in dataframe.columns if dataframe[col].dtype != \"O\"]\n    num_cols = [col for col in num_cols if col not in num_but_cat]\n    print(f\"Observations: {dataframe.shape[0]}\")\n    print(f\"Variables: {dataframe.shape[1]}\")\n    print(f'cat_cols: {len(cat_cols)}')\n    print(f'num_cols: {len(num_cols)}')\n    print(f'cat_but_car: {len(cat_but_car)}')\n    print(f'num_but_cat: {len(num_but_cat)}')\n    return cat_cols, cat_but_car, num_cols\n\ndef cat_summary(dataframe, col_name, plot=False):\n    print(pd.DataFrame({col_name: dataframe[col_name].value_counts(),\n                        \"Ratio\": 100 * dataframe[col_name].value_counts() / len(dataframe)}))\n    if plot:\n        sns.countplot(x=dataframe[col_name], data=dataframe)\n        plt.xticks(rotation=45)\n        plt.savefig(f'{col_name}_countplot.png')\n        plt.close()\n\ndef num_summary(dataframe, numerical_col, plot=False):\n    quantiles = [0.05, 0.10, 0.50, 0.90, 0.95, 0.99]\n    print(dataframe[numerical_col].describe(quantiles).T)\n    if plot:\n        dataframe[numerical_col].hist(bins=50)\n        plt.xlabel(numerical_col)\n        plt.title(numerical_col)\n        plt.savefig(f'{numerical_col}_histogram.png')\n        plt.close()\n    print(\"#####################################\")\n\ndef target_summary_with_cat(dataframe, target, categorical_col):\n    print(pd.DataFrame({\"TARGET_MEAN\": dataframe.groupby(categorical_col)[target].mean()}), end=\"\\n\\n\\n\")\n\ndef high_correlated_cols(dataframe, plot=False, corr_th=CORRELATION_THRESHOLD):\n    corr = dataframe.corr(numeric_only=True)\n    cor_matrix = corr.abs()\n    upper_triangle_matrix = cor_matrix.where(np.triu(np.ones(cor_matrix.shape), k=1).astype(bool))\n    drop_list = [col for col in upper_triangle_matrix.columns if any(upper_triangle_matrix[col] > corr_th)]\n    if plot:\n        sns.heatmap(corr, cmap=\"RdBu\", annot=True)\n        plt.savefig('correlation_heatmap.png')\n        plt.close()\n    return drop_list\n\ndef outlier_thresholds(dataframe, variable, low_quantile=LOW_QUANTILE, up_quantile=UP_QUANTILE):\n    q1 = dataframe[variable].quantile(low_quantile)\n    q3 = dataframe[variable].quantile(up_quantile)\n    iqr = q3 - q1\n    lower_bound = q1 - 1.5 * iqr\n    upper_bound = q3 + 1.5 * iqr\n    return lower_bound, upper_bound\n\ndef check_outlier(dataframe, col_name):\n    low_limit, up_limit = outlier_thresholds(dataframe, col_name)\n    return dataframe[(dataframe[col_name] > up_limit) | (dataframe[col_name] < low_limit)].any(axis=None)\n\ndef replace_with_thresholds(dataframe, variable):\n    low_limit, up_limit = outlier_thresholds(dataframe, variable)\n    dataframe.loc[dataframe[variable] < low_limit, variable] = low_limit\n    dataframe.loc[dataframe[variable] > up_limit, variable] = up_limit\n    return dataframe\n\ndef missing_values_table(dataframe, na_name=False):\n    na_columns = [col for col in dataframe.columns if dataframe[col].isnull().sum() > 0]\n    n_miss = dataframe[na_columns].isnull().sum().sort_values(ascending=False)\n    ratio = (dataframe[na_columns].isnull().sum() / dataframe.shape[0] * 100).sort_values(ascending=False)\n    missing_df = pd.concat([n_miss, np.round(ratio, 2)], axis=1, keys=['n_miss', 'ratio'])\n    print(missing_df, end=\"\\n\")\n    if na_name:\n        return na_columns\n\ndef remove_missing_values(dataframe):\n    print(\"##################### Missing Values Before #####################\")\n    print(dataframe.isnull().sum())\n    dataframe_cleaned = dataframe.dropna()\n    print(\"##################### Missing Values After #####################\")\n    print(dataframe_cleaned.isnull().sum())\n    return dataframe_cleaned\n\ndef quick_missing_imp(data, num_method=NUM_METHOD, cat_length=CAT_LENGTH, target=\"Age\"):\n    variables_with_na = [col for col in data.columns if data[col].isnull().sum() > 0]\n    temp_target = data[target] if target in data.columns else None\n    print(\"# BEFORE\")\n    print(data[variables_with_na].isnull().sum(), \"\\n\")\n    data = data.apply(lambda x: x.fillna(x.mode()[0]) if (x.dtype == \"O\" and len(x.unique()) <= cat_length) else x, axis=0)\n    if num_method == \"mean\":\n        data = data.apply(lambda x: x.fillna(x.mean()) if x.dtype != \"O\" else x, axis=0)\n    elif num_method == \"median\":\n        data = data.apply(lambda x: x.fillna(x.median()) if x.dtype != \"O\" else x, axis=0)\n    if temp_target is not None:\n        data[target] = temp_target\n    print(\"# AFTER\")\n    print(\"Categorical variables filled with mode\")\n    print(f\"Numerical variables filled with {num_method}\")\n    print(data[variables_with_na].isnull().sum(), \"\\n\")\n    return data\n\ndef rare_analyser(dataframe, target, cat_cols):\n    for col in cat_cols:\n        print(col, \":\", len(dataframe[col].value_counts()))\n        print(pd.DataFrame({\"COUNT\": dataframe[col].value_counts(),\n                            \"RATIO\": dataframe[col].value_counts() / len(dataframe),\n                            \"TARGET_MEAN\": dataframe.groupby(col)[target].mean()}), end=\"\\n\\n\\n\")\n\ndef rare_encoder(dataframe, rare_perc):\n    temp_df = dataframe.copy()\n    rare_columns = [col for col in temp_df.columns if temp_df[col].dtype == 'O'\n                    and (temp_df[col].value_counts() / len(temp_df) < rare_perc).any(axis=None)]\n    for var in rare_columns:\n        tmp = temp_df[var].value_counts() / len(temp_df)\n        rare_labels = tmp[tmp < rare_perc].index\n        temp_df[var] = np.where(temp_df[var].isin(rare_labels), 'Rare', temp_df[var])\n    return temp_df\n\ndef label_encoder(dataframe, binary_col):\n    labelencoder = LabelEncoder()\n    dataframe[binary_col] = labelencoder.fit_transform(dataframe[binary_col])\n    return dataframe\n\ndef one_hot_encoder(dataframe, categorical_cols, drop_first=False):\n    return pd.get_dummies(dataframe, columns=categorical_cols, drop_first=drop_first)",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T06:32:18.131289790Z",
     "start_time": "2025-05-05T06:15:28.021386Z"
    }
   },
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "source": "# %% [markdown]\n\"\"\"\n## Data Loading\n\"\"\"\n# %%\n# DataLoader class (equivalent to dataset.py)\nclass DataLoader:\n    def __init__(self, training_data_path, testing_data_path):\n        self.training_data_path = training_data_path\n        self.testing_data_path = testing_data_path\n        print(\"Initializing DataLoader...\")\n\n    def get_data(self):\n        print(\"Loading data...\")\n        # Load and combine data\n        train = pd.read_csv(self.training_data_path)\n        test = pd.read_csv(self.testing_data_path)\n        df = pd.concat([train, test], ignore_index=True)\n        df = df.reset_index(drop=True)\n        print(\"Data loaded successfully.\")\n        return df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T06:32:18.131635675Z",
     "start_time": "2025-05-05T06:15:28.072908Z"
    }
   },
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "source": "# %% [markdown]\n\"\"\"\n## Data Preprocessing\n\"\"\"\n# %%\n# DataPreprocessing class (equivalent to data_preprocessing.py)\nclass DataPreprocessing:\n    def __init__(self, dataframe):\n        \"\"\"\n        Initialize with a combined DataFrame (train + test).\n        \"\"\"\n        self.df = dataframe.copy()\n\n    def preprocess(self, is_test_only=False):\n        \"\"\"\n        Preprocess the data and return train/validation splits or test data.\n        \"\"\"\n        self.handle_outliers()\n        self.handle_missing_values()\n        self.feature_engineering()\n        self.drop_unnecessary_columns()\n        self.encode_features()\n\n        if is_test_only:\n            # Test verisini al ve 'id'yi dÃ¼ÅŸÃ¼rmeden Ã¶nce sakla\n            test_data = self.df[self.df['Ã¼rÃ¼n fiyatÄ±'].isnull()].drop('Ã¼rÃ¼n fiyatÄ±', axis=1)\n            test_ids = test_data[\"id\"].copy()  # 'id'yi sakla\n            test_data = test_data.drop(columns=['id'])  # 'id'yi test verisinden Ã§Ä±kar\n            return test_data, test_ids  # 'test_ids' ile birlikte dÃ¶ndÃ¼r\n        else:\n            # EÄŸitim verisini al ve 'id'yi dÃ¼ÅŸÃ¼r\n            train_data = self.df[self.df['Ã¼rÃ¼n fiyatÄ±'].notnull()]\n            train_data = train_data.drop(columns=['id'])  # 'id'yi dÃ¼ÅŸÃ¼r\n\n            X = train_data.drop('Ã¼rÃ¼n fiyatÄ±', axis=1)\n            y = train_data['Ã¼rÃ¼n fiyatÄ±']\n            X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=42)\n            return X_train, X_val, y_train, y_val\n\n    def handle_outliers(self):\n        \"\"\"\n        Handle outliers using IQR method for numerical columns (excluding Ã¼rÃ¼n fiyatÄ±).\n        \"\"\"\n        num_cols = self.df.select_dtypes(include=np.number).columns\n        num_cols = [col for col in num_cols if col != 'Ã¼rÃ¼n fiyatÄ±']  # Hedef deÄŸiÅŸkeni hariÃ§ tut\n        for col in num_cols:\n            if check_outlier(self.df, col):\n                self.df = replace_with_thresholds(self.df, col)\n\n    def handle_missing_values(self):\n        \"\"\"\n        Handle missing values: mean for numerical (excluding Ã¼rÃ¼n fiyatÄ±), mode for categorical.\n        \"\"\"\n        num_cols = self.df.select_dtypes(include=np.number).columns\n        num_cols = [col for col in num_cols if col != 'Ã¼rÃ¼n fiyatÄ±']\n        self.df[num_cols] = self.df[num_cols].fillna(self.df[num_cols].mean())\n        cat_cols = self.df.select_dtypes(include='object').columns\n        for col in cat_cols:\n            self.df[col] = self.df[col].fillna(self.df[col].mode()[0])\n\n    def feature_engineering(self):\n        \"\"\"\n        Create new features for the dataset.\n        \"\"\"\n        # Besin deÄŸeri ile ilgili Ã¶zellikler\n        self.df['besin_deÄŸeri_log'] = np.log1p(self.df['Ã¼rÃ¼n besin deÄŸeri'])  # Log dÃ¶nÃ¼ÅŸÃ¼mÃ¼\n\n        # ÃœrÃ¼n kategorisi bazlÄ± ortalama besin deÄŸeri\n        self.df['kategori_ortalama_besin'] = self.df.groupby('Ã¼rÃ¼n kategorisi')['Ã¼rÃ¼n besin deÄŸeri'].transform('mean')\n\n    def drop_unnecessary_columns(self):\n        \"\"\"\n        Drop unnecessary columns.\n        \"\"\"\n        columns_to_drop = ['Ã¼rÃ¼n Ã¼retim yeri', 'market', 'ÅŸehir']  # Tek deÄŸerli sÃ¼tunlar\n        self.df.drop(columns=[col for col in columns_to_drop if col in self.df.columns], inplace=True)\n\n    def encode_features(self):\n        \"\"\"\n        Encode categorical features (Ã¼rÃ¼n, Ã¼rÃ¼n kategorisi).\n        \"\"\"\n        cat_cols, cat_but_car, num_cols = grab_col_names(self.df)\n\n        # Binary veya dÃ¼ÅŸÃ¼k kardinaliteli sÃ¼tunlar iÃ§in label encoding\n        binary_cols = [col for col in cat_cols if self.df[col].nunique() <= 3]  # Ã–rneÄŸin, Ã¼rÃ¼n kategorisi\n        for col in binary_cols:\n            self.df = label_encoder(self.df, col)\n\n        # YÃ¼ksek kardinaliteli sÃ¼tunlar (Ã¶rneÄŸin, Ã¼rÃ¼n) iÃ§in target encoding\n        high_cardinality_cols = cat_but_car + [col for col in cat_cols if col not in binary_cols]\n        for col in high_cardinality_cols:\n            if col in self.df.columns:\n                # Train verisi iÃ§in hedef ortalamasÄ± hesapla\n                train_data = self.df[self.df['Ã¼rÃ¼n fiyatÄ±'].notnull()]\n                target_means = train_data.groupby(col)['Ã¼rÃ¼n fiyatÄ±'].mean()\n                # TÃ¼m veriye ortalamalarÄ± uygula, bilinmeyen deÄŸerler iÃ§in genel ortalama\n                self.df[col] = self.df[col].map(target_means).fillna(train_data['Ã¼rÃ¼n fiyatÄ±'].mean())\n\n        # Kalan kategorik sÃ¼tunlar iÃ§in one-hot encoding\n        remaining_cat_cols = [col for col in cat_cols if col not in binary_cols and col not in high_cardinality_cols]\n        if remaining_cat_cols:\n            self.df = one_hot_encoder(self.df, remaining_cat_cols, drop_first=True)\n\n        # Hala kategorik sÃ¼tun kalmÄ±ÅŸsa hata fÄ±rlat\n        remaining_object_cols = self.df.select_dtypes(include='object').columns.tolist()\n        if remaining_object_cols:\n            raise ValueError(f\"Categorical columns not fully encoded: {remaining_object_cols}\")",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T06:32:18.151948998Z",
     "start_time": "2025-05-05T06:15:28.126984Z"
    }
   },
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "source": [
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## Hyperparameter Tuning\n",
    "\"\"\"\n",
    "# %%\n",
    "# HyperTuner class (equivalent to HyperTuner.py)\n",
    "class HyperTuner:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize with a dictionary of models and their hyperparameter grids.\n",
    "        \"\"\"\n",
    "        self.param_grids = {\n",
    "            \"LinearRegression\": {},\n",
    "            \"Ridge\": {'alpha': [0.1, 1.0, 10.0, 100.0]},\n",
    "            \"Lasso\": {'alpha': [0.01, 0.1, 1.0, 10.0]},\n",
    "            \"ElasticNet\": {'alpha': [0.01, 0.1, 1.0], 'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]},\n",
    "            \"KNN\": {'n_neighbors': [3, 5, 7, 9], 'weights': ['uniform', 'distance']},\n",
    "            \"DecisionTree\": {'max_depth': [3, 5, 10, None], 'min_samples_split': [2, 5, 10]},\n",
    "            \"RandomForest\": {\n",
    "                'n_estimators': randint(60,160),\n",
    "                'max_depth': [30, None],\n",
    "                'min_samples_split': [5, 10,15],\n",
    "                'min_samples_leaf': [1, 2, 4,6]\n",
    "            },\n",
    "            \"GradientBoosting\": {\n",
    "                'n_estimators': randint(50, 200),\n",
    "                'learning_rate': loguniform(0.005, 0.2),\n",
    "                'max_depth': [3, 5, 7],\n",
    "                'subsample': uniform(0.6, 0.4),\n",
    "                'min_samples_split': [2, 5, 10]\n",
    "            },\n",
    "            \"XGBoost\": {\n",
    "                'n_estimators': randint(50, 200),\n",
    "                'learning_rate': loguniform(0.005, 0.2),\n",
    "                'max_depth': [3, 5, 7, 9],\n",
    "                'subsample': uniform(0.6, 0.4),\n",
    "                'colsample_bytree': uniform(0.6, 0.4)\n",
    "            },\n",
    "            \"LightGBM\": {\n",
    "                'n_estimators': randint(50, 200),\n",
    "                'learning_rate': loguniform(0.005, 0.2),\n",
    "                'num_leaves': randint(20, 50),\n",
    "                'max_depth': [3, 5, 7, -1],\n",
    "                'subsample': uniform(0.6, 0.4),\n",
    "                'colsample_bytree': uniform(0.6, 0.4)\n",
    "            },\n",
    "            \"CatBoost\": {\n",
    "    'iterations': randint(200, 1000),  # daha geniÅŸ aralÄ±k, genellikle 500-1000 iyi sonuÃ§ verir\n",
    "    'learning_rate': loguniform(0.01, 0.1),  # Ã§ok kÃ¼Ã§Ã¼k deÄŸerler Ã¶ÄŸrenmeyi yavaÅŸlatÄ±r\n",
    "    'depth': [4, 6, 8],  # aÅŸÄ±rÄ± derinlik overfitting yapabilir\n",
    "    'l2_leaf_reg': loguniform(1, 10),  # dÃ¼zenleme terimi\n",
    "    'bagging_temperature': uniform(0, 1),  # Ã¶rnekleme Ã§eÅŸitliliÄŸi\n",
    "    'random_strength': uniform(1, 10),  # kategorik veri ayrÄ±mÄ±nda rassallÄ±k\n",
    "    'border_count': randint(32, 128),  # sayÄ±sal verileri ayrÄ±ÅŸtÄ±rma detay seviyesi\n",
    "    'grow_policy': ['SymmetricTree', 'Depthwise', 'Lossguide']\n",
    "            },\n",
    "            \"SVR\": {\n",
    "                'C': loguniform(0.1, 10),\n",
    "                'epsilon': uniform(0.05, 0.2),\n",
    "                'kernel': ['rbf', 'linear'],\n",
    "                'gamma': loguniform(1e-4, 1e-1)\n",
    "            }\n",
    "        }\n",
    "        self.models = {\n",
    "            \"LinearRegression\": LinearRegression(),\n",
    "            \"Ridge\": Ridge(),\n",
    "            \"Lasso\": Lasso(),\n",
    "            \"ElasticNet\": ElasticNet(),\n",
    "            \"KNN\": KNeighborsRegressor(),\n",
    "            \"DecisionTree\": DecisionTreeRegressor(),\n",
    "            \"RandomForest\": RandomForestRegressor(random_state=17),\n",
    "            \"GradientBoosting\": GradientBoostingRegressor(random_state=17),\n",
    "            \"XGBoost\": XGBRegressor(objective='reg:squarederror', random_state=17),\n",
    "            \"LightGBM\": LGBMRegressor(random_state=17),\n",
    "            \"CatBoost\": CatBoostRegressor(silent=True, random_state=17),\n",
    "        }\n",
    "\n",
    "    def tune_model(self, model_name, X, y):\n",
    "        \"\"\"\n",
    "        Tune the specified model using GridSearchCV or RandomizedSearchCV.\n",
    "        \"\"\"\n",
    "        if model_name not in self.models:\n",
    "            raise ValueError(f\"Model {model_name} not found in models list.\")\n",
    "\n",
    "        model = self.models[model_name]\n",
    "        param_grid = self.param_grids[model_name]\n",
    "\n",
    "        if param_grid:\n",
    "            # Use RandomizedSearchCV for complex models\n",
    "            search = RandomizedSearchCV(\n",
    "                estimator=model,\n",
    "                param_distributions=param_grid,\n",
    "                n_iter=20,\n",
    "                cv=5,\n",
    "                scoring='neg_mean_absolute_error',\n",
    "                n_jobs=-1,\n",
    "                random_state=42\n",
    "            )\n",
    "            search.fit(X, y)\n",
    "            return search.best_estimator_, search.best_params_\n",
    "        else:\n",
    "            model.fit(X, y)\n",
    "            return model, {}"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T06:32:18.152500927Z",
     "start_time": "2025-05-05T06:15:28.179719Z"
    }
   },
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "source": "# %% [markdown]\n\"\"\"\n## Model Evaluation\n\"\"\"\n# %%\n# ModelEvaluator class (equivalent to models.py)\nclass ModelEvaluator:\n    def __init__(self, best_model_name=\"CatBoost\", output_dir=\"predictions/notebook\"):\n        \"\"\"\n        Initialize with a list of regression model names and specify the best model for final predictions.\n        \"\"\"\n        self.model_names = [\n            \"CatBoost\",\n        ]\n        self.best_model_name = best_model_name\n        self.rmse_scores = {}\n        self.mae_scores = {}\n        self.best_params = {}\n        self.tuner = HyperTuner()\n        self.trained_models = {}\n        self.output_dir = output_dir\n        os.makedirs(self.output_dir, exist_ok=True)\n\n    def evaluate_models(self, X, y):\n        \"\"\"\n        Evaluate all models with hyperparameter tuning using 5-fold cross-validation.\n        \"\"\"\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=17)\n\n        print(\"Evaluating models with hyperparameter tuning...\")\n        for name in self.model_names:\n            # Tune and train the model\n            best_model, best_params = self.tuner.tune_model(name, X_train, y_train)\n            self.trained_models[name] = best_model\n            self.best_params[name] = best_params\n\n            # Calculate RMSE\n            rmse = np.mean(np.sqrt(-cross_val_score(best_model, X, y, cv=5, scoring=\"neg_mean_squared_error\")))\n            self.rmse_scores[name] = rmse\n\n            # Calculate MAE\n            mae = np.mean(-cross_val_score(best_model, X, y, cv=5, scoring=\"neg_mean_absolute_error\"))\n            self.mae_scores[name] = mae\n\n            print(f\"RMSE: {round(rmse, 4)} | MAE: {round(mae, 4)} ({name})\")\n            if self.best_params[name]:\n                print(f\"Best parameters: {self.best_params[name]}\")\n\n        # Print the best model based on MAE\n        best_model_mae = min(self.mae_scores, key=self.mae_scores.get)\n        print(f\"\\nBest model based on MAE: {best_model_mae} (MAE: {round(self.mae_scores[best_model_mae], 4)})\")\n        print(f\"Best parameters for {best_model_mae}: {self.best_params[best_model_mae]}\")\n\n        # Print the best model based on RMSE for reference\n        best_model_rmse = min(self.rmse_scores, key=self.rmse_scores.get)\n        print(f\"Best model based on RMSE: {best_model_rmse} (RMSE: {round(self.rmse_scores[best_model_rmse], 4)})\")\n\n        return X_train, X_test, y_train, y_test\n\n    def train_and_predict(self, X_train, y_train, X_test, test_ids, output_file=\"submission.csv\"):\n        \"\"\"\n        Train the best model with optimized parameters and save predictions to CSV.\n\n        Args:\n            X_train (pd.DataFrame): Training features\n            y_train (pd.Series): Training target values\n            X_test (pd.DataFrame): Test features to predict on\n            test_ids (pd.Series): IDs for test samples\n            output_file (str): Output CSV filename\n\n        Returns:\n            np.ndarray: Model predictions\n        \"\"\"\n        # Model training with verbose output\n        print(f\"\\nâ³ Training {self.best_model_name} model...\")\n        start_time = time.time()\n\n        # Hyperparameter tuning and training\n        best_model, best_params = self.tuner.tune_model(self.best_model_name, X_train, y_train)\n        self.best_params[self.best_model_name] = best_params\n\n        # Train final model\n        best_model.fit(X_train, y_train)\n        training_time = time.time() - start_time\n        print(f\"âœ… Training completed in {training_time:.2f} seconds\")\n        print(f\"ðŸ† Best parameters: {best_params}\")\n\n        # Generate predictions - keep as floating point without rounding\n        predictions = best_model.predict(X_test)\n\n        # Create submission DataFrame with floating point prices\n        submission_df = pd.DataFrame({\n            \"id\": test_ids.astype(int),\n            \"Ã¼rÃ¼n fiyatÄ±\": predictions.astype(float)  # Ensure floating point type\n        })\n\n        # Save to CSV without index\n        output_path = os.path.join(self.output_dir, output_file)\n        submission_df.to_csv(output_path, index=False, float_format='%.4f')  # 4 decimal places\n\n        print(f\"\\nðŸ“ Predictions saved to '{output_path}'\")\n        print(f\"Sample predictions:\\n{submission_df.head()}\")\n\n        return predictions\n\n    def get_rmse_scores(self):\n        \"\"\"\n        Return the RMSE scores for all evaluated models.\n        \"\"\"\n        return self.rmse_scores\n\n    def get_mae_scores(self):\n        \"\"\"\n        Return the MAE scores for all evaluated models.\n        \"\"\"\n        return self.mae_scores\n\n    def get_best_params(self):\n        \"\"\"\n        Return the best parameters for all evaluated models.\n        \"\"\"\n        return self.best_params",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T06:32:18.152945688Z",
     "start_time": "2025-05-05T06:15:28.238120Z"
    }
   },
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "source": [
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## Main Execution\n",
    "\"\"\"\n",
    "# %%\n",
    "# Main function (equivalent to main.py)\n",
    "def main():\n",
    "    # Initialize timer\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(\"ðŸš€ Starting product price prediction pipeline...\")\n",
    "\n",
    "    # 1. Data Loading\n",
    "    print(\"\\nðŸ“‚ Loading data...\")\n",
    "    data_loader = DataLoader(TRAINING_DATA_PATH, TESTING_DATA_PATH)\n",
    "    combined_df = data_loader.get_data()\n",
    "    print(f\"âœ… Data loaded. Shape: {combined_df.shape}\")\n",
    "\n",
    "    # 2. Data Preprocessing\n",
    "    print(\"\\nðŸ”§ Preprocessing data...\")\n",
    "    preprocessor = DataPreprocessing(combined_df)\n",
    "\n",
    "    # Training/validation split\n",
    "    X_train, X_val, y_train, y_val = preprocessor.preprocess()\n",
    "    print(f\"âœ… Training data prepared. Features: {X_train.shape[1]}, Samples: {X_train.shape[0]}\")\n",
    "\n",
    "    # Test data preparation\n",
    "    X_test_submission, test_ids = preprocessor.preprocess(is_test_only=True)\n",
    "    print(f\"âœ… Test data prepared. Samples: {X_test_submission.shape[0]}\")\n",
    "\n",
    "    # 3. Model Evaluation\n",
    "    print(\"\\nðŸ§ª Evaluating models...\")\n",
    "    evaluator = ModelEvaluator(output_dir=\"predictions/Pipeline\")\n",
    "    evaluator.evaluate_models(X_train, y_train)\n",
    "\n",
    "    # 4. Final Prediction\n",
    "    print(\"\\nðŸ”® Making final predictions...\")\n",
    "    predictions = evaluator.train_and_predict(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_test_submission,\n",
    "        test_ids,\n",
    "        output_file=\"submission.csv\"\n",
    "    )\n",
    "\n",
    "    # Pipeline completion\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nðŸŽ‰ Pipeline completed in {total_time:.2f} seconds!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import time\n",
    "    main()"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T06:32:18.049635Z",
     "start_time": "2025-05-05T06:15:28.291020Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting product price prediction pipeline...\n",
      "\n",
      "ðŸ“‚ Loading data...\n",
      "Initializing DataLoader...\n",
      "Loading data...\n",
      "Data loaded successfully.\n",
      "âœ… Data loaded. Shape: (273024, 9)\n",
      "\n",
      "ðŸ”§ Preprocessing data...\n",
      "Observations: 273024\n",
      "Variables: 8\n",
      "cat_cols: 2\n",
      "num_cols: 4\n",
      "cat_but_car: 2\n",
      "num_but_cat: 1\n",
      "âœ… Training data prepared. Features: 6, Samples: 170640\n",
      "Observations: 273024\n",
      "Variables: 8\n",
      "cat_cols: 2\n",
      "num_cols: 6\n",
      "cat_but_car: 0\n",
      "num_but_cat: 2\n",
      "âœ… Test data prepared. Samples: 45504\n",
      "\n",
      "ðŸ§ª Evaluating models...\n",
      "Evaluating models with hyperparameter tuning...\n",
      "RMSE: 1.3725 | MAE: 0.7626 (CatBoost)\n",
      "Best parameters: {'bagging_temperature': np.float64(0.4667628932479799), 'border_count': 82, 'depth': 8, 'grow_policy': 'SymmetricTree', 'iterations': 366, 'l2_leaf_reg': np.float64(1.0310149462998892), 'learning_rate': np.float64(0.0875390351800604), 'random_strength': np.float64(6.632882178455393)}\n",
      "\n",
      "Best model based on MAE: CatBoost (MAE: 0.7626)\n",
      "Best parameters for CatBoost: {'bagging_temperature': np.float64(0.4667628932479799), 'border_count': 82, 'depth': 8, 'grow_policy': 'SymmetricTree', 'iterations': 366, 'l2_leaf_reg': np.float64(1.0310149462998892), 'learning_rate': np.float64(0.0875390351800604), 'random_strength': np.float64(6.632882178455393)}\n",
      "Best model based on RMSE: CatBoost (RMSE: 1.3725)\n",
      "\n",
      "ðŸ”® Making final predictions...\n",
      "\n",
      "â³ Training CatBoost model...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[24], line 52\u001B[0m\n\u001B[1;32m     50\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtime\u001B[39;00m\n\u001B[0;32m---> 52\u001B[0m     \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[24], line 38\u001B[0m, in \u001B[0;36mmain\u001B[0;34m()\u001B[0m\n\u001B[1;32m     36\u001B[0m \u001B[38;5;66;03m# 4. Final Prediction\u001B[39;00m\n\u001B[1;32m     37\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mðŸ”® Making final predictions...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 38\u001B[0m predictions \u001B[38;5;241m=\u001B[39m \u001B[43mevaluator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_and_predict\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     39\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     40\u001B[0m \u001B[43m    \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     41\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX_test_submission\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     42\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtest_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     43\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_file\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43msubmission.csv\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\n\u001B[1;32m     44\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     46\u001B[0m \u001B[38;5;66;03m# Pipeline completion\u001B[39;00m\n\u001B[1;32m     47\u001B[0m total_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m-\u001B[39m start_time\n",
      "Cell \u001B[0;32mIn[23], line 79\u001B[0m, in \u001B[0;36mModelEvaluator.train_and_predict\u001B[0;34m(self, X_train, y_train, X_test, test_ids, output_file)\u001B[0m\n\u001B[1;32m     76\u001B[0m start_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[1;32m     78\u001B[0m \u001B[38;5;66;03m# Hyperparameter tuning and training\u001B[39;00m\n\u001B[0;32m---> 79\u001B[0m best_model, best_params \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtuner\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtune_model\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbest_model_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     80\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbest_params[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbest_model_name] \u001B[38;5;241m=\u001B[39m best_params\n\u001B[1;32m     82\u001B[0m \u001B[38;5;66;03m# Train final model\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[22], line 99\u001B[0m, in \u001B[0;36mHyperTuner.tune_model\u001B[0;34m(self, model_name, X, y)\u001B[0m\n\u001B[1;32m     88\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m param_grid:\n\u001B[1;32m     89\u001B[0m     \u001B[38;5;66;03m# Use RandomizedSearchCV for complex models\u001B[39;00m\n\u001B[1;32m     90\u001B[0m     search \u001B[38;5;241m=\u001B[39m RandomizedSearchCV(\n\u001B[1;32m     91\u001B[0m         estimator\u001B[38;5;241m=\u001B[39mmodel,\n\u001B[1;32m     92\u001B[0m         param_distributions\u001B[38;5;241m=\u001B[39mparam_grid,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     97\u001B[0m         random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m\n\u001B[1;32m     98\u001B[0m     )\n\u001B[0;32m---> 99\u001B[0m     \u001B[43msearch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    100\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m search\u001B[38;5;241m.\u001B[39mbest_estimator_, search\u001B[38;5;241m.\u001B[39mbest_params_\n\u001B[1;32m    101\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m~/miniconda3/envs/BTK-Manisa/lib/python3.10/site-packages/sklearn/base.py:1389\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[0;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1382\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[1;32m   1384\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[1;32m   1385\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[1;32m   1386\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[1;32m   1387\u001B[0m     )\n\u001B[1;32m   1388\u001B[0m ):\n\u001B[0;32m-> 1389\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/BTK-Manisa/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1024\u001B[0m, in \u001B[0;36mBaseSearchCV.fit\u001B[0;34m(self, X, y, **params)\u001B[0m\n\u001B[1;32m   1018\u001B[0m     results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_results(\n\u001B[1;32m   1019\u001B[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001B[1;32m   1020\u001B[0m     )\n\u001B[1;32m   1022\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m results\n\u001B[0;32m-> 1024\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_search\u001B[49m\u001B[43m(\u001B[49m\u001B[43mevaluate_candidates\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1026\u001B[0m \u001B[38;5;66;03m# multimetric is determined here because in the case of a callable\u001B[39;00m\n\u001B[1;32m   1027\u001B[0m \u001B[38;5;66;03m# self.scoring the return type is only known after calling\u001B[39;00m\n\u001B[1;32m   1028\u001B[0m first_test_score \u001B[38;5;241m=\u001B[39m all_out[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest_scores\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[0;32m~/miniconda3/envs/BTK-Manisa/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1951\u001B[0m, in \u001B[0;36mRandomizedSearchCV._run_search\u001B[0;34m(self, evaluate_candidates)\u001B[0m\n\u001B[1;32m   1949\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_run_search\u001B[39m(\u001B[38;5;28mself\u001B[39m, evaluate_candidates):\n\u001B[1;32m   1950\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001B[39;00m\n\u001B[0;32m-> 1951\u001B[0m     \u001B[43mevaluate_candidates\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1952\u001B[0m \u001B[43m        \u001B[49m\u001B[43mParameterSampler\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1953\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparam_distributions\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mn_iter\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrandom_state\u001B[49m\n\u001B[1;32m   1954\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1955\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/BTK-Manisa/lib/python3.10/site-packages/sklearn/model_selection/_search.py:970\u001B[0m, in \u001B[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001B[0;34m(candidate_params, cv, more_results)\u001B[0m\n\u001B[1;32m    962\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    963\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\n\u001B[1;32m    964\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFitting \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m folds for each of \u001B[39m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;124m candidates,\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    965\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m totalling \u001B[39m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m fits\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m    966\u001B[0m             n_splits, n_candidates, n_candidates \u001B[38;5;241m*\u001B[39m n_splits\n\u001B[1;32m    967\u001B[0m         )\n\u001B[1;32m    968\u001B[0m     )\n\u001B[0;32m--> 970\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mparallel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    971\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdelayed\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_fit_and_score\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    972\u001B[0m \u001B[43m        \u001B[49m\u001B[43mclone\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbase_estimator\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    973\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    974\u001B[0m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    975\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrain\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    976\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtest\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    977\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparameters\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparameters\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    978\u001B[0m \u001B[43m        \u001B[49m\u001B[43msplit_progress\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43msplit_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_splits\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    979\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcandidate_progress\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcand_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_candidates\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    980\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfit_and_score_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    981\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    982\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mcand_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparameters\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43msplit_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mproduct\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    983\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcandidate_params\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    984\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mrouted_params\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplitter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplit\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    985\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    986\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    988\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(out) \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m    989\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    990\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo fits were performed. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    991\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWas the CV iterator empty? \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    992\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWere there no candidates?\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    993\u001B[0m     )\n",
      "File \u001B[0;32m~/miniconda3/envs/BTK-Manisa/lib/python3.10/site-packages/sklearn/utils/parallel.py:77\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[0;34m(self, iterable)\u001B[0m\n\u001B[1;32m     72\u001B[0m config \u001B[38;5;241m=\u001B[39m get_config()\n\u001B[1;32m     73\u001B[0m iterable_with_config \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m     74\u001B[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001B[1;32m     75\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m delayed_func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m iterable\n\u001B[1;32m     76\u001B[0m )\n\u001B[0;32m---> 77\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43miterable_with_config\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/BTK-Manisa/lib/python3.10/site-packages/joblib/parallel.py:2007\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[0;34m(self, iterable)\u001B[0m\n\u001B[1;32m   2001\u001B[0m \u001B[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001B[39;00m\n\u001B[1;32m   2002\u001B[0m \u001B[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001B[39;00m\n\u001B[1;32m   2003\u001B[0m \u001B[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001B[39;00m\n\u001B[1;32m   2004\u001B[0m \u001B[38;5;66;03m# dispatch of the tasks to the workers.\u001B[39;00m\n\u001B[1;32m   2005\u001B[0m \u001B[38;5;28mnext\u001B[39m(output)\n\u001B[0;32m-> 2007\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m output \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreturn_generator \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43moutput\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/BTK-Manisa/lib/python3.10/site-packages/joblib/parallel.py:1650\u001B[0m, in \u001B[0;36mParallel._get_outputs\u001B[0;34m(self, iterator, pre_dispatch)\u001B[0m\n\u001B[1;32m   1647\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m\n\u001B[1;32m   1649\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend\u001B[38;5;241m.\u001B[39mretrieval_context():\n\u001B[0;32m-> 1650\u001B[0m         \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_retrieve()\n\u001B[1;32m   1652\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mGeneratorExit\u001B[39;00m:\n\u001B[1;32m   1653\u001B[0m     \u001B[38;5;66;03m# The generator has been garbage collected before being fully\u001B[39;00m\n\u001B[1;32m   1654\u001B[0m     \u001B[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001B[39;00m\n\u001B[1;32m   1655\u001B[0m     \u001B[38;5;66;03m# the user if necessary.\u001B[39;00m\n\u001B[1;32m   1656\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/BTK-Manisa/lib/python3.10/site-packages/joblib/parallel.py:1762\u001B[0m, in \u001B[0;36mParallel._retrieve\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1757\u001B[0m \u001B[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001B[39;00m\n\u001B[1;32m   1758\u001B[0m \u001B[38;5;66;03m# async callbacks to progress.\u001B[39;00m\n\u001B[1;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ((\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jobs) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m\n\u001B[1;32m   1760\u001B[0m     (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jobs[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mget_status(\n\u001B[1;32m   1761\u001B[0m         timeout\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtimeout) \u001B[38;5;241m==\u001B[39m TASK_PENDING)):\n\u001B[0;32m-> 1762\u001B[0m     \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1763\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[1;32m   1765\u001B[0m \u001B[38;5;66;03m# We need to be careful: the job list can be filling up as\u001B[39;00m\n\u001B[1;32m   1766\u001B[0m \u001B[38;5;66;03m# we empty it and Python list are not thread-safe by\u001B[39;00m\n\u001B[1;32m   1767\u001B[0m \u001B[38;5;66;03m# default hence the use of the lock\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 24
  }
 ]
}
